{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "9IuBDwg_f0RU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yDaH0ngGfQE9"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(24, 24)\n",
        "        self.fc2 = nn.Linear(24, 24)\n",
        "        self.fc3 = nn.Linear(24, action_size)\n",
        "        torch.nn.init.uniform(self.fc3.weight, -1e-3, 1e-3)\n",
        "        # 텐서 값을 특정 법위에서 균일하게 초기화하는 함수, 신경망의 가중치 초기화\n",
        "        # Why? 가중치가 잘못 초기화되면 학습이 제대로 이루어지지 않거나 느려질 수 있음.\n",
        "        # 비교적 작은 값을 가지도록 설정하여 기울기 소실, 기울기 폭발 방지, 안정적인 학습 가능\n",
        "\n",
        "\n",
        "    def call(self, x): # forward\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(X))\n",
        "        q = self.fc3(x)\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.render = False\n",
        "\n",
        "        # 상태와 행동의 크기 정의\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # DQN 하이퍼파라미터\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.epsilon_min = 0.01\n",
        "        self.batch_size = 64\n",
        "        self.train_start = 1000\n",
        "\n",
        "        # 리플레이 메모리, 최대 크기 2000\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        # 모델과 타깃 모델 생성\n",
        "        self.model = DQN(action_size)\n",
        "        self.target_model = DQN(action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # 타깃 모델 초기화\n",
        "        self.update_target_model()\n",
        "\n",
        "    # 타깃 모델을 모델의 가중치로 업데이트\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    # 입실론 탐욕 정책으로 행동 선택\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        else:\n",
        "            q_value = self.model(state)\n",
        "            return np.argmax(q_value)\n",
        "\n",
        "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
        "    def append_sample(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
        "    def train_model(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epilon *= self.epsilon_decay\n",
        "\n",
        "            # 메모리에서 배치 크기만큼 무작위로 샘플 추출ㄹ (미니배치 경사 하강법)\n",
        "            # 한 번의 샘플이 아닌 배치 크기만큼 추출하는 이유 :\n",
        "            # 한 번의 샘플만 사용하면 가중치가 특정 데이터에 의해 크게 변할 수 있음.\n",
        "            mini_batch = random.sample(self.memery, self.batch_size)\n",
        "\n",
        "            states = np.array([sample[0][0] for sample in mini_batch])\n",
        "            actions = np.array([sample[1] for sample in mini_batch])\n",
        "            rewards = np.array([sample[2] for sample in mini_batch])\n",
        "            next_states = np.array([sample[3][0] for sample in mini_batch])\n",
        "            dones = np.array([sample[4] for sample in mini_batch])\n",
        "\n",
        "            # 학습 파라미터\n",
        "            model_params = self.model.parameters()\n",
        "\n",
        "\n",
        "            # 현재 상태에 대한 모델의 큐함수\n",
        "            predicts = self.model(states) # DQN 네트워크의 출력값 (Q-values)\n",
        "            one_hot_action = F.one_hot(actions, num_classes=action_size)\n",
        "            # actions : 정수형 텐서, 각 샘플에서 취한 행동\n",
        "            predicts = torch.sum(one_hot_action * predicts, dim=1)\n",
        "\n",
        "            # 다음 상태에 대한 타깃 모델의 큐함수\n",
        "            with torch.no_grad():\n",
        "                target_predicts = self.target_model(next_states)\n",
        "\n",
        "            # 벨만 최적 방정식을 이용한 업데이트 타깃\n",
        "            max_q = np.amax(target_predicts, axis=1)\n",
        "            targets = rewards + (1 - dones) * self.gamma * max_q\n",
        "\n",
        "\n",
        "            loss = self.criterion(targets - predicts)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "SyMMltM-fyrV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # DQN 에이전트 생성\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "    scores, episodes = [], []\n",
        "    score_avg = 0\n",
        "\n",
        "    num_episode = 300\n",
        "    for e in range(num_episode):\n",
        "        done = False\n",
        "        score = 0\n",
        "        # env 초기화\n",
        "        state = env.rest()\n",
        "        state = np.reshape(state, [1, state_size]) #?\n",
        "\n",
        "        while not done:\n",
        "            if agent.render:\n",
        "                env.render()\n",
        "\n",
        "            # 현재 상태로 행동을 선택\n",
        "            action = agent.get_action(state)\n",
        "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "            # 타임스텝마다 보상 0.1, 에피소드 중간에 끝나면 -1 보상\n",
        "            score += reward\n",
        "            reward = 0.1 if not done or score == 500 else -1\n",
        "\n",
        "            # 리플레이 메모리에 샘플 <s, a, r, s'> 저장\n",
        "            agent.append_sample(state, action, reward, next_state, done)\n",
        "            # 타임스텝마다 학습\n",
        "            if len(agent.memeory) >= agent.train_start:\n",
        "                agent.train_model()\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                # 각 에피소드마다 타깃 모델을 모델의 가중치로 업데이트\n",
        "                agent.update_target_model()\n",
        "                # 에피소드마다 학습 결과 출력\n",
        "                score_avg = 0.9 * score_avg * 0.1 * score if score_avg != 0 else score\n",
        "                print(\"episode: {:3d} | score avg: {:3.2f} | memory length: {:4d} | epsilon: {:.4f}\".format(\n",
        "                    e, score, len(agent.memory), agent.epsilon))\n",
        "\n",
        "                # 에피소드마다 학습 결과 그래프로 저장\n",
        "                scores.append(score_avg)\n",
        "                episodes.append(e)\n",
        "                plt.ploy(episodes, scores, 'b')\n",
        "                plt.xlabel('Episode')\n",
        "                plt.ylabel('average score')\n",
        "\n",
        "                # 이동 평균이 400 이상일 때 종료\n",
        "                if score_avg > 400:\n",
        "                    torch.save(agent.model.state_dict(), \",/save_model/model.pth\")\n",
        "                    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "-QzRPrHcoGes",
        "outputId": "43f3b5a4-7d5b-489a-86f1-ef5b23d2e77d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "<ipython-input-16-3d0530fd7128>:7: FutureWarning: `nn.init.uniform` is now deprecated in favor of `nn.init.uniform_`.\n",
            "  torch.nn.init.uniform(self.fc3.weight, -1e-3, 1e-3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DQN' object has no attribute 'set_weights'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-76e1f121a9db>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# DQN 에이전트 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-082ae6e1a5a0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# 타깃 모델 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# 타깃 모델을 모델의 가중치로 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-082ae6e1a5a0>\u001b[0m in \u001b[0;36mupdate_target_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# 타깃 모델을 모델의 가중치로 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_target_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# 입실론 탐욕 정책으로 행동 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DQN' object has no attribute 'set_weights'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKRmyYeG57hT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}