{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "lEroSal8ux2M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [[0 for _ in range(3)] for _ in range(3)]\n",
        "        self.current_player = 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [[0 for _ in range(3)] for _ in range(3)]\n",
        "        self.current_player = 1\n",
        "\n",
        "    def render(self):\n",
        "        players = {1: 'X', -1: 'O', 0: ' '}\n",
        "        for i, row in enumerate(self.board):\n",
        "            print(\" \" + \" | \".join([players[cell] for cell in row]))\n",
        "            if i < 2:\n",
        "                print(\"---+---+---\")\n",
        "\n",
        "    def available_moves(self):\n",
        "        return [(r, c) for r in range(3) for c in range(3) if self.board[r][c] == 0]\n",
        "\n",
        "    def make_move(self, row, col):\n",
        "        if self.board[row][col] == 0:\n",
        "            self.board[row][col] = self.current_player\n",
        "            self.current_player *= -1\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_winner(self):\n",
        "        board = np.array(self.board)\n",
        "        row_sum = np.sum(board, axis=1)\n",
        "        col_sum = np.sum(board, axis=0)\n",
        "        diag1_sum = np.trace(board)\n",
        "        diag2_sum = np.trace(np.fliplr(board))\n",
        "        all_sums = np.concatenate((row_sum, col_sum, [diag1_sum], [diag2_sum]))\n",
        "\n",
        "        if 3 in all_sums:\n",
        "            return 1\n",
        "        if -3 in all_sums:\n",
        "            return -1\n",
        "        if not (board == 0).any():\n",
        "            return 0\n",
        "        return None\n",
        "\n",
        "    def clone(self):\n",
        "        cloned_game = TicTacToe()\n",
        "        cloned_game.board = [row[:] for row in self.board]\n",
        "        cloned_game.current_player = self.current_player\n",
        "        return cloned_game\n",
        "\n",
        "    def random_play(self):\n",
        "        while self.check_winner() is None:\n",
        "            moves = self.available_moves()\n",
        "            move = random.choice(moves)\n",
        "            self.make_move(*move)\n"
      ],
      "metadata": {
        "id": "BifejpdTEJa_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oAnfLO1KvL2W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 0.99\n",
        "EPSILON = 1.0\n",
        "EPSILON_MIN = 0.1\n",
        "EPSILON_DECAY = 0.995\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "UPDATE_TIME = 100"
      ],
      "metadata": {
        "id": "0EZ_UyXy5qr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qOHBjuK_uZm1"
      },
      "outputs": [],
      "source": [
        "class DDQNAgent:\n",
        "    def __init__(self):\n",
        "        self.gamma = GAMMA\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "        self.epsilon_decay = EPSILON_DECAY\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.batch_size = BATCH_SIZE\n",
        "\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = DQN(9, 9).to(self.device)\n",
        "        self.target_model = DQN(9, 9).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        self.update_time = UPDATE_TIME\n",
        "        self.update_target_network()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def append_samples(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(range(9))\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            state = torch.FloatTensor(state).to(self.device)\n",
        "            next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "\n",
        "            target = reward\n",
        "            if not done:\n",
        "                next_action = torch.argmax(self.model(next_state)).item()\n",
        "                target = reward + self.gamma * self.target_model(next_state)[next_action].item()\n",
        "\n",
        "            target_f = self.model(state)\n",
        "            target_f[action] = target\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = nn.MSELoss()(self.model(state), target_f.detach())\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "if __name__ == \"__main__\":\n",
        "    agent = DDQNAgent()\n",
        "    episodes = 1000\n",
        "\n",
        "    for e in range(episodes):\n",
        "        game = TicTacToe()\n",
        "        state = np.array(game.board).flatten()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            row, col = divmod(action, 3)\n",
        "            if not game.make_move(row, col):\n",
        "                continue\n",
        "\n",
        "            reward = 0\n",
        "            winner = game.check_winner()\n",
        "            if winner is not None:\n",
        "                done = True\n",
        "                if winner == 1:\n",
        "                    reward = 1\n",
        "                elif winner == -1:\n",
        "                    reward = -1\n",
        "                else:\n",
        "                    reward = 0.3\n",
        "\n",
        "            next_state = np.array(game.board).flatten()\n",
        "            agent.append_samples(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "\n",
        "        agent.replay()\n",
        "        if e % 100 == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        if e % 100 == 0:\n",
        "            print(f\"Episode {e}/{episodes} - Epsilon: {agent.epsilon}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJqu7-CQulTT",
        "outputId": "abec9d6a-2a68-41bd-a6f2-2223d50cfba5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/1000 - Epsilon: 1.0\n",
            "Episode 100/1000 - Epsilon: 0.6274028820538087\n",
            "Episode 200/1000 - Epsilon: 0.3800621177172763\n",
            "Episode 300/1000 - Epsilon: 0.23023039494318503\n",
            "Episode 400/1000 - Epsilon: 0.13946676683816583\n",
            "Episode 500/1000 - Epsilon: 0.0996820918179746\n",
            "Episode 600/1000 - Epsilon: 0.0996820918179746\n",
            "Episode 700/1000 - Epsilon: 0.0996820918179746\n",
            "Episode 800/1000 - Epsilon: 0.0996820918179746\n",
            "Episode 900/1000 - Epsilon: 0.0996820918179746\n"
          ]
        }
      ]
    }
  ]
}