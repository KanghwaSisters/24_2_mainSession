{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/jackdawkins11/pytorch-alpha-zero"
      ],
      "metadata": {
        "id": "mvsgRD6e8sOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 게임 상태 준비"
      ],
      "metadata": {
        "id": "7TGi9sIYDtqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 임포트\n",
        "import random\n",
        "import math\n",
        "\n",
        "# 게임 상태\n",
        "class State:\n",
        "    # 초기화\n",
        "    def __init__(self, pieces=None, enemy_pieces=None):\n",
        "        # 돌의 배치\n",
        "        self.pieces = pieces if pieces != None else [0] * 9\n",
        "        self.enemy_pieces = enemy_pieces if enemy_pieces != None else [0] * 9\n",
        "\n",
        "    # 돌의 수 얻기\n",
        "    def piece_count(self, pieces):\n",
        "        return sum(1 for i in pieces if i == 1)\n",
        "\n",
        "    # 패배 여부 판정\n",
        "    def is_lose(self):\n",
        "        # 돌 3개 연결 여부 확인\n",
        "        def is_comp(x, y, dx, dy):\n",
        "            for k in range(3):\n",
        "                if y < 0 or 2 < y or x < 0 or 2 < x or \\\n",
        "                        self.enemy_pieces[x + y * 3] == 0:\n",
        "                    return False\n",
        "                x, y = x + dx, y + dy\n",
        "            return True\n",
        "\n",
        "        # 패배 여부 판정\n",
        "        if is_comp(0, 0, 1, 1) or is_comp(0, 2, 1, -1):\n",
        "            return True\n",
        "        for i in range(3):\n",
        "            if is_comp(0, i, 1, 0) or is_comp(i, 0, 0, 1):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # 무승부 여부 확인\n",
        "    def is_draw(self):\n",
        "        return self.piece_count(self.pieces) + self.piece_count(self.enemy_pieces) == 9\n",
        "\n",
        "    # 게임 종료 여부 확인\n",
        "    def is_done(self):\n",
        "        return self.is_lose() or self.is_draw()\n",
        "\n",
        "    # 다음 상태 얻기\n",
        "    def next(self, action):\n",
        "        pieces = self.pieces.copy()\n",
        "        pieces[action] = 1\n",
        "        return State(self.enemy_pieces, pieces)\n",
        "\n",
        "    # 합법적인 수의 리스트 얻기\n",
        "    def legal_actions(self):\n",
        "        return [i for i in range(9) if self.pieces[i] == 0 and self.enemy_pieces[i] == 0]\n",
        "\n",
        "    # 선 수 여부 확인\n",
        "    def is_first_player(self):\n",
        "        return self.piece_count(self.pieces) == self.piece_count(self.enemy_pieces)\n",
        "\n",
        "    # 문자열 표시\n",
        "    def __str__(self):\n",
        "        ox = ('o', 'x') if self.is_first_player() else ('x', 'o')\n",
        "        str = ''\n",
        "        for i in range(9):\n",
        "            if self.pieces[i] == 1:\n",
        "                str += ox[0]\n",
        "            elif self.enemy_pieces[i] == 1:\n",
        "                str += ox[1]\n",
        "            else:\n",
        "                str += '-'\n",
        "            if i % 3 == 2:\n",
        "                str += '\\n'\n",
        "        return str\n",
        "\n",
        "\n",
        "# 랜덤으로 행동 선택\n",
        "def random_action(state):\n",
        "    legal_actions = state.legal_actions()\n",
        "    return legal_actions[random.randint(0, len(legal_actions) - 1)]\n",
        "\n",
        "\n",
        "# 알파베타법을 활용한 상태 가치 계산\n",
        "def alpha_beta(state, alpha, beta):\n",
        "    # 패배 시 상태 가치 -1\n",
        "    if state.is_lose():\n",
        "        return -1\n",
        "\n",
        "    # 무승부 시, 상테 가치 0\n",
        "    if state.is_draw():\n",
        "        return 0\n",
        "\n",
        "    # 합법적인 수의 상태 가치 계산\n",
        "    for action in state.legal_actions():\n",
        "        score = -alpha_beta(state.next(action), -beta, -alpha)\n",
        "        if score > alpha:\n",
        "            alpha = score\n",
        "\n",
        "        # 현재 노드의 베스트 스코어가 부모 노드보다 크면 탐색 종료\n",
        "        if alpha >= beta:\n",
        "            return alpha\n",
        "\n",
        "    # 합법적인 수의 강태 가치의 최대값을 반환\n",
        "    return alpha\n",
        "\n",
        "\n",
        "# 알파베타법을 활용한 생동 선택\n",
        "def alpha_beta_action(state):\n",
        "    # 합법적인 수의 상태 가치 계산\n",
        "    best_action = 0\n",
        "    alpha = -float('inf')\n",
        "    for action in state.legal_actions():\n",
        "        score = -alpha_beta(state.next(action), -float('inf'), -alpha)\n",
        "        if score > alpha:\n",
        "            best_action = action\n",
        "            alpha = score\n",
        "\n",
        "    # 합법적인 수의 상태 가치의 최대값을 갖는 행동 반환\n",
        "    return best_action\n",
        "\n",
        "\n",
        "# 플레이아웃\n",
        "def playout(state):\n",
        "    # 패배 시, 상태 가치 -1\n",
        "    if state.is_lose():\n",
        "        return -1\n",
        "\n",
        "    # 무승부 시, 상태 가치 0\n",
        "    if state.is_draw():\n",
        "        return 0\n",
        "\n",
        "    # 다음 상태의 상태 가치\n",
        "    return -playout(state.next(random_action(state)))\n",
        "\n",
        "\n",
        "# 최대값의 인덱스 반환\n",
        "def argmax(collection):\n",
        "    return collection.index(max(collection))\n",
        "\n",
        "\n",
        "# 몬테카를로 트리 탐색을 활용한 행동 선택\n",
        "def mcts_action(state):\n",
        "    # 몬테카를로 트리 탐색 노드\n",
        "    class node:\n",
        "        # 초기화\n",
        "        def __init__(self, state):\n",
        "            self.state = state  # 상태\n",
        "            self.w = 0  # 가치 누계\n",
        "            self.n = 0  # 시행 횟수\n",
        "            self.child_nodes = None  # 자녀 노드군\n",
        "\n",
        "        # 평가\n",
        "        def evaluate(self):\n",
        "            # 게임 종료 시\n",
        "            if self.state.is_done():\n",
        "                # 승패 결과로 가치 얻기\n",
        "                value = -1 if self.state.is_lose() else 0  # 패배 시 -1, 무승부 시 0\n",
        "\n",
        "                # 가치 누계와 시행 횟수 갱신\n",
        "                self.w += value\n",
        "                self.n += 1\n",
        "                return value\n",
        "\n",
        "            # 자녀 노드가 존재하지 않는 경우\n",
        "            if not self.child_nodes:\n",
        "                # 플레이아웃으로 가치 얻기\n",
        "                value = playout(self.state)\n",
        "\n",
        "                # 가치 누계와 시행 횟수 갱신\n",
        "                self.w += value\n",
        "                self.n += 1\n",
        "\n",
        "                # 자녀 노드 전개\n",
        "                if self.n == 10:\n",
        "                    self.expand()\n",
        "                return value\n",
        "\n",
        "            # 자녀 노드가 존재하는 경우\n",
        "            else:\n",
        "                # UCB1이 가장 큰 자녀 노드를 평가해 가치 얻기\n",
        "                value = -self.next_child_node().evaluate()\n",
        "\n",
        "                # 보상 누계와 시행 횟수 갱신\n",
        "                self.w += value\n",
        "                self.n += 1\n",
        "                return value\n",
        "\n",
        "        # 자녀 노드 전개\n",
        "        def expand(self):\n",
        "            legal_actions = self.state.legal_actions()\n",
        "            self.child_nodes = []\n",
        "            for action in legal_actions:\n",
        "                self.child_nodes.append(node(self.state.next(action)))\n",
        "\n",
        "        # UCB1가 가장 큰 자녀 노드 얻기\n",
        "        def next_child_node(self):\n",
        "            # 시행 횟수 n이 0인 자녀 노드를 반환\n",
        "            for child_node in self.child_nodes:\n",
        "                if child_node.n == 0:\n",
        "                    return child_node\n",
        "\n",
        "            # UCB1 계산\n",
        "            t = 0\n",
        "            for c in self.child_nodes:\n",
        "                t += c.n\n",
        "            ucb1_values = []\n",
        "            for child_node in self.child_nodes:\n",
        "                ucb1_values.append(-child_node.w / child_node.n + 2 * (2 * math.log(t) / child_node.n) ** 0.5)\n",
        "\n",
        "            # UCB1가 가장 큰 자녀 노드를 반환\n",
        "            return self.child_nodes[argmax(ucb1_values)]\n",
        "\n",
        "    # 루트 노드 생성\n",
        "    root_node = node(state)\n",
        "    root_node.expand()\n",
        "\n",
        "    # 루트 노드를 100회 평가\n",
        "    for _ in range(100):\n",
        "        root_node.evaluate()\n",
        "\n",
        "    # 시행 횟수 최대값을 갖는 행동 반환\n",
        "    legal_actions = state.legal_actions()\n",
        "    n_list = []\n",
        "    for c in root_node.child_nodes:\n",
        "        n_list.append(c.n)\n",
        "    return legal_actions[argmax(n_list)]\n",
        "\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    # 상태 생성\n",
        "    state = State()\n",
        "\n",
        "    # 게임 종료 시까지 반복\n",
        "    while True:\n",
        "        # 게임 종료 시\n",
        "        if state.is_done():\n",
        "            break\n",
        "\n",
        "        # 다음 상태 얻기\n",
        "        state = state.next(random_action(state))\n",
        "\n",
        "        # 문자열 표시\n",
        "        print(state)\n",
        "        print()"
      ],
      "metadata": {
        "id": "TJR0_Z0m8sw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc9d1cd-8c83-48f2-e2d6-624a48e72f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "---\n",
            "o--\n",
            "\n",
            "\n",
            "-x-\n",
            "---\n",
            "o--\n",
            "\n",
            "\n",
            "-xo\n",
            "---\n",
            "o--\n",
            "\n",
            "\n",
            "xxo\n",
            "---\n",
            "o--\n",
            "\n",
            "\n",
            "xxo\n",
            "---\n",
            "oo-\n",
            "\n",
            "\n",
            "xxo\n",
            "---\n",
            "oox\n",
            "\n",
            "\n",
            "xxo\n",
            "--o\n",
            "oox\n",
            "\n",
            "\n",
            "xxo\n",
            "-xo\n",
            "oox\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 듀얼 네트워크 생성"
      ],
      "metadata": {
        "id": "G4TBDXOrFoVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# 파라미터 준비\n",
        "FILTERS = 128  # 컨볼루션 레이어 커널 수(오리지널 256）\n",
        "RESIDUAL_NUM = 16  # 레지듀얼 블록 수(오리지널 19)\n",
        "INPUT_SHAPE = (3,3,2)  # 입력 셰이프\n",
        "OUTPUT_SIZE = 9  # 행동 수(배치 수(3*3))\n",
        "\n",
        "# 레지듀얼 블록 생성\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, filters):\n",
        "        super(ResidualBlock,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(filters, filters, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(filters)\n",
        "        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(filters)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sc = x\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        x += sc\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# 듀얼 네트워크 생성\n",
        "class DualNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, output_size, filters=128, residual_num=16):\n",
        "        super(DualNetwork, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.input_conv = nn.Conv2d(input_shape[0], filters, kernel_size=3, padding=1, bias=False)\n",
        "        self.input_bn = nn.BatchNorm2d(filters)\n",
        "        self.residual_blocks = nn.Sequential(*[ResidualBlock(filters) for _ in range(residual_num)])\n",
        "        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.policy_head = nn.Linear(filters, output_size)\n",
        "        self.value_head = nn.Linear(filters, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.input_bn(self.input_conv(x)))\n",
        "        x = self.residual_blocks(x)\n",
        "        x = self.global_pooling(x)  # [batch_size, filters, 1, 1]\n",
        "        x = x.view(x.size(0), -1)  # [batch_size, filters]\n",
        "\n",
        "        # Policy head\n",
        "        policy = F.softmax(self.policy_head(x), dim=1)\n",
        "\n",
        "        # Value head\n",
        "        value = torch.tanh(self.value_head(x))\n",
        "\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "# 모델 저장 함수\n",
        "def save_dual_network(model, path='./model/best.pth'):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "# 모델 로드 함수\n",
        "def load_dual_network(input_shape, output_size, filters=128, residual_num=16, path='./model/best.pth'):\n",
        "    model = DualNetwork(input_shape, output_size, filters, residual_num)\n",
        "    if os.path.exists(path):\n",
        "        model.load_state_dict(torch.load(path))\n",
        "    return model\n",
        "\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    # 모델 생성\n",
        "    input_shape =INPUT_SHAPE # [channels, height, width]\n",
        "    output_size = OUTPUT_SIZE\n",
        "    model = DualNetwork(input_shape, output_size, FILTERS, RESIDUAL_NUM)\n",
        "\n",
        "    # 모델 저장\n",
        "    save_dual_network(model)\n",
        "\n",
        "    # 모델 로드\n",
        "    loaded_model = load_dual_network(input_shape, output_size)\n",
        "    print(\"모델이 성공적으로 생성 및 저장/로드되었습니다.\")"
      ],
      "metadata": {
        "id": "4m2SsZ74DzVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb34c52-719e-4a12-96ea-063e64e7b2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델이 성공적으로 생성 및 저장/로드되었습니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-6b0a1caceda2>:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 몬테카를로 트리 탐색 구현"
      ],
      "metadata": {
        "id": "fm_eqLNcMFW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 임포트\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from math import sqrt\n",
        "\n",
        "# 파라미터 준비\n",
        "PV_EVALUATE_COUNT = 50  # 추론 1회당 시뮬레이션 횟수(오리지널: 1600회)\n",
        "\n",
        "\n",
        "# 추론\n",
        "def predict(model, state, device):\n",
        "    # 추론을 위한 입력 데이터 셰이프 변환\n",
        "    a, b, c = INPUT_SHAPE\n",
        "    x = np.array([state.pieces, state.enemy_pieces])\n",
        "    x = x.reshape(c, a, b).transpose(1, 2, 0).reshape(1, a, b, c)\n",
        "\n",
        "    # 텐서로 변환 및 GPU 이동\n",
        "    x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 추론\n",
        "    with torch.no_grad():\n",
        "        policy, value = model(x)\n",
        "\n",
        "    # 정책 얻기\n",
        "    policies = policy.squeeze(0).cpu().numpy()[list(state.legal_actions())]  # 합법적인 수만 선택\n",
        "    policies /= sum(policies) if sum(policies) else 1  # 합계 1의 확률 분포로 변환\n",
        "\n",
        "    # 가치 얻기\n",
        "    value = value.squeeze(0).item()\n",
        "    return policies, value\n",
        "\n",
        "# 노드 리스트를 시행 횟수 리스트로 변환\n",
        "def nodes_to_scores(nodes):\n",
        "    scores = [node.n for node in nodes]\n",
        "    return scores\n",
        "\n",
        "\n",
        "# 몬테카를로 트리 탐색 스코어 얻기\n",
        "def pv_mcts_scores(model, state, temperature, device):\n",
        "    # 몬테카를로 트리 탐색 노드 정의\n",
        "    class Node:\n",
        "        # 노드 초기화\n",
        "        def __init__(self, state, p):\n",
        "            self.state = state  # 상태\n",
        "            self.p = p  # 정책\n",
        "            self.w = 0  # 가치 누계\n",
        "            self.n = 0  # 시행 횟수\n",
        "            self.child_nodes = None  # 자녀 노드군\n",
        "\n",
        "        # 국면 가치 계산\n",
        "        def evaluate(self):\n",
        "            # 게임 종료 시\n",
        "            if self.state.is_done():\n",
        "                # 승패 결과로 가치 얻기\n",
        "                value = -1 if self.state.is_lose() else 0\n",
        "\n",
        "                # 누계 가치와 시행 횟수 갱신\n",
        "                self.w += value\n",
        "                self.n += 1\n",
        "                return value\n",
        "\n",
        "            # 자녀 노드가 존재하지 않는 경우\n",
        "            if not self.child_nodes:\n",
        "                # 뉴럴 네트워크 추론을 활용한 정책과 가치 얻기\n",
        "                policies, value = predict(model, self.state, device)\n",
        "\n",
        "                # 누계 가치와 시행 횟수 갱신\n",
        "                self.w += value\n",
        "                self.n += 1\n",
        "\n",
        "                # 자녀 노드 전개\n",
        "                self.child_nodes = []\n",
        "                for action, policy in zip(self.state.legal_actions(), policies):\n",
        "                    self.child_nodes.append(Node(self.state.next(action), policy))\n",
        "                return value\n",
        "\n",
        "            # 자녀 노드가 존재하지 않는 경우\n",
        "            else:\n",
        "                # 아크 평가값이 가장 큰 자녀 노드의 평가로 가치 얻기\n",
        "                value = -self.next_child_node().evaluate()\n",
        "\n",
        "                # 누계 가치와 시행 횟수 갱신\n",
        "                self.w += value\n",
        "                self.n += 1\n",
        "                return value\n",
        "\n",
        "        # 아크 평가가 가장 큰 자녀 노드 얻기\n",
        "        def next_child_node(self):\n",
        "            # 아크 평가 계산\n",
        "            C_PUCT = 1.0\n",
        "            t = sum(nodes_to_scores(self.child_nodes))\n",
        "            pucb_values = []\n",
        "            for child_node in self.child_nodes:\n",
        "                pucb_values.append((-child_node.w / child_node.n if child_node.n else 0.0) +\n",
        "                                   C_PUCT * child_node.p * sqrt(t) / (1 + child_node.n))\n",
        "\n",
        "            # 아크 평가값이 가장 큰 자녀 노드 반환\n",
        "            return self.child_nodes[np.argmax(pucb_values)]\n",
        "\n",
        "    # 현재 국면의 노드 생성\n",
        "    root_node = Node(state, 0)\n",
        "\n",
        "    # 여러 차례 평가 실행\n",
        "    for _ in range(PV_EVALUATE_COUNT):\n",
        "        root_node.evaluate()\n",
        "\n",
        "    # 합법적인 수의 확률 분포\n",
        "    scores = nodes_to_scores(root_node.child_nodes)\n",
        "    if temperature == 0:  # 최대값인 경우에만 1\n",
        "        action = np.argmax(scores)\n",
        "        scores = np.zeros(len(scores))\n",
        "        scores[action] = 1\n",
        "    else:  # 볼츠만 분포를 기반으로 분산 추가\n",
        "        scores = boltzman(scores, temperature)\n",
        "    return scores\n",
        "\n",
        "\n",
        "# 몬테카를로 트리 탐색을 활용한 행동 선택\n",
        "def pv_mcts_action(model, temperature, device='cuda'):\n",
        "    def pv_mcts_action(state):\n",
        "        scores = pv_mcts_scores(model, state, temperature, device)\n",
        "        return np.random.choice(state.legal_actions(), p=scores)\n",
        "\n",
        "    return pv_mcts_action\n",
        "\n",
        "\n",
        "# 볼츠만 분포\n",
        "def boltzman(xs, temperature):\n",
        "    xs = [x ** (1 / temperature) for x in xs]\n",
        "    return [x / sum(xs) for x in xs]\n",
        "\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    # GPU/CPU 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 모델 로드\n",
        "    model_path = sorted(Path('./model').glob('*.pth'))[-1]\n",
        "    model = DualNetwork(INPUT_SHAPE, OUTPUT_SIZE, filters=128, residual_num=16)  # 모델 파라미터 설정\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 상태 생성\n",
        "    state = State()\n",
        "\n",
        "    # 몬테카를로 트리 탐색을 활용해 행동을 얻는 함수 생성\n",
        "    next_action = pv_mcts_action(model, temperature=1.0, device=device)\n",
        "\n",
        "    # 게임 종료 시까지 반복\n",
        "    while True:\n",
        "        # 게임 종료 시\n",
        "        if state.is_done():\n",
        "            break\n",
        "\n",
        "        # 행동 얻기\n",
        "        action = next_action(state)\n",
        "\n",
        "        # 다음 상태 얻기\n",
        "        state = state.next(action)\n",
        "\n",
        "        # 문자열 출력\n",
        "        print(state)"
      ],
      "metadata": {
        "id": "j4hqM2vELP9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d346178-1fb7-4860-9f40-34b475e6cbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-b34977230ecb>:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "---\n",
            "o--\n",
            "\n",
            "---\n",
            "-x-\n",
            "o--\n",
            "\n",
            "---\n",
            "-x-\n",
            "o-o\n",
            "\n",
            "---\n",
            "-xx\n",
            "o-o\n",
            "\n",
            "---\n",
            "oxx\n",
            "o-o\n",
            "\n",
            "x--\n",
            "oxx\n",
            "o-o\n",
            "\n",
            "x--\n",
            "oxx\n",
            "ooo\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# self play"
      ],
      "metadata": {
        "id": "NEYsOl0Qmtmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 파라미터 준비\n",
        "SP_GAME_COUNT = 10  # 셀프 플레이를 수행할 게임 수(오리지널: 25,000)\n",
        "\n",
        "\n",
        "\n",
        "# 선 수를 둔 플레이어 가치\n",
        "def first_player_value(ended_state):\n",
        "    # 1: 선 수 플레이어 승리, -1: 선 수 플레이어 패배, 0: 무승부\n",
        "    if ended_state.is_lose():\n",
        "        return -1 if ended_state.is_first_player() else 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "# 학습 데이터 저장\n",
        "def write_data(history):\n",
        "    now = datetime.now()\n",
        "    os.makedirs('./data/', exist_ok=True)  # 폴더가 없는 경우에는 생성\n",
        "    path = './data/{:04}{:02}{:02}{:02}{:02}{:02}.history'.format(\n",
        "        now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
        "    with open(path, mode='wb') as f:\n",
        "        pickle.dump(history, f)\n",
        "\n",
        "\n",
        "# 1 게임 실행\n",
        "def play(model):\n",
        "    # 학습 데이터\n",
        "    history = []\n",
        "\n",
        "    # 상태 생성\n",
        "    state = State()\n",
        "\n",
        "    while True:\n",
        "        # 게임 종료 시\n",
        "        if state.is_done():\n",
        "            break\n",
        "\n",
        "        # 합법적인 수의 확률 분포 얻기\n",
        "        scores = pv_mcts_scores(model, state, temperature=1.0, device=device)\n",
        "\n",
        "        # 학습 데이터에 상태와 정책 추가\n",
        "        policies = [0] * OUTPUT_SIZE\n",
        "        for action, policy in zip(state.legal_actions(), scores):\n",
        "            policies[action] = policy\n",
        "        history.append([[state.pieces, state.enemy_pieces], policies, None])\n",
        "\n",
        "        # 행동 얻기\n",
        "        action = np.random.choice(state.legal_actions(), p=scores)\n",
        "\n",
        "        # 다음 상태 얻기\n",
        "        state = state.next(action)\n",
        "\n",
        "    # 학습 데이터에 가치 추가\n",
        "    value = first_player_value(state)\n",
        "    for i in range(len(history)):\n",
        "        history[i][2] = value\n",
        "        value = -value\n",
        "    return history\n",
        "\n",
        "\n",
        "# 셀프 플레이\n",
        "def self_play():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # 학습 데이터\n",
        "    history = []\n",
        "\n",
        "    # 베스트 플레이어 모델 로드\n",
        "    model = DualNetwork(INPUT_SHAPE, OUTPUT_SIZE, filters=128, residual_num=16)\n",
        "    model.to(device)\n",
        "\n",
        "    # 여러 차례 게임 실행\n",
        "    for i in range(SP_GAME_COUNT):\n",
        "        # 게임 1회 실행\n",
        "        h = play(model)\n",
        "        history.extend(h)\n",
        "\n",
        "        # 출력\n",
        "        print('\\rSelfPlay {}/{}'.format(i+1, SP_GAME_COUNT), end='')\n",
        "    print('')\n",
        "\n",
        "    # 학습 데이터 저장\n",
        "    write_data(history)\n",
        "\n",
        "    # 모델 파기\n",
        "    del model\n",
        "\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    self_play()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw9W6RkSkGbL",
        "outputId": "3b23fd6f-56a4-4acb-8423-e5b5ed2e704d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfPlay 10/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 파라미터 갱신"
      ],
      "metadata": {
        "id": "cCL3uu9TpiaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch.optim.lr_scheduler import StepLR  # 학습률 스케줄러 임포트\n",
        "\n",
        "# 파라미터 준비\n",
        "RN_EPOCHS = 100  # 학습 횟수\n",
        "BATCH_SIZE = 128  # 배치 크기\n",
        "\n",
        "# 학습 데이터 로드\n",
        "def load_data():\n",
        "    history_path = sorted(Path('./data').glob('*.history'))[-1]\n",
        "    with history_path.open(mode='rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "# 듀얼 네트워크 학습\n",
        "def train_network():\n",
        "    # 학습 데이터 로드\n",
        "    history = load_data()\n",
        "    xs, y_policies, y_values = zip(*history)\n",
        "\n",
        "    # 학습을 위한 입력 데이터 셰이프로 변환\n",
        "    a, b, c = INPUT_SHAPE\n",
        "    xs = np.array(xs)\n",
        "    xs = xs.reshape(len(xs), c, a, b).transpose(0, 2, 3, 1)\n",
        "    y_policies = np.array(y_policies)\n",
        "    y_values = np.array(y_values)\n",
        "\n",
        "    # PyTorch 텐서로 변환\n",
        "    xs = torch.tensor(xs, dtype=torch.float32)\n",
        "    y_policies = torch.tensor(y_policies, dtype=torch.float32)\n",
        "    y_values = torch.tensor(y_values, dtype=torch.float32)\n",
        "\n",
        "    dataset = TensorDataset(xs, y_policies, y_values)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # 베스트 플레이어 모델 로드\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = DualNetwork(INPUT_SHAPE, OUTPUT_SIZE, filters=128, residual_num=16)\n",
        "    # model = torch.load('./model/best.pth')  # .h5 -> .pth로 변경\n",
        "\n",
        "    # 모델을 GPU로 이동\n",
        "    # model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    # 손실 함수 설정\n",
        "    criterion_policy = torch.nn.CrossEntropyLoss()\n",
        "    criterion_value = torch.nn.MSELoss()\n",
        "\n",
        "    # 학습률 스케줄러 설정\n",
        "    lr_scheduler = StepLR(optimizer, step_size=50, gamma=0.5)  # 50번 에포크마다 학습률을 0.5배로 줄임\n",
        "\n",
        "    # 학습 시작\n",
        "    for epoch in range(RN_EPOCHS):\n",
        "        model.train()\n",
        "        running_loss_policy = 0.0\n",
        "        running_loss_value = 0.0\n",
        "        for batch_idx, (inputs, policies, values) in enumerate(dataloader):\n",
        "            # 옵티마이저 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 모델 예측\n",
        "            policy_output, value_output = model(inputs)\n",
        "\n",
        "            # 손실 계산\n",
        "            loss_policy = criterion_policy(policy_output, policies)\n",
        "            loss_value = criterion_value(value_output, values)\n",
        "\n",
        "            # 전체 손실 계산\n",
        "            loss = loss_policy + loss_value\n",
        "\n",
        "            # 역전파\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 손실 출력\n",
        "            running_loss_policy += loss_policy.item()\n",
        "            running_loss_value += loss_value.item()\n",
        "\n",
        "        # 학습률 조정\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # 매 epoch 출력\n",
        "        print(f'\\rTrain {epoch + 1}/{RN_EPOCHS}, Loss Policy: {running_loss_policy:.4f}, Loss Value: {running_loss_value:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}', end='')\n",
        "\n",
        "    print('')\n",
        "\n",
        "    # 최신 플레이어 모델 저장\n",
        "    torch.save(model, './model/latest.pth')  # .h5 -> .pth로 변경\n",
        "\n",
        "    # 모델 파기\n",
        "    del model\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    train_network()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYu_j17onlN2",
        "outputId": "f9d872e3-dab6-4079-d0f1-aa77caa0c406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([66])) that is different to the input size (torch.Size([66, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 100/100, Loss Policy: 1.9135, Loss Value: 1.5909, LR: 0.000250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 신규 파라미터 평가 파트"
      ],
      "metadata": {
        "id": "hkS6USrsC_BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from shutil import copy\n",
        "import numpy as np\n",
        "\n",
        "# 파라미터 준비\n",
        "EN_GAME_COUNT = 10  # 평가 1회 당 게임 수(오리지널: 400)\n",
        "EN_TEMPERATURE = 1.0  # 볼츠만 분포 온도\n",
        "\n",
        "\n",
        "# 선 수를 둔 플레이어의 포인트\n",
        "def first_player_point(ended_state):\n",
        "    # 1: 선 수 플레이어 승리, 0: 선 수 플레이어 패배, 0.5: 무승부\n",
        "    if ended_state.is_lose():\n",
        "        return 0 if ended_state.is_first_player() else 1\n",
        "    return 0.5\n",
        "\n",
        "\n",
        "# 1 게임 실행\n",
        "def play(next_actions):\n",
        "    # 상태 생성\n",
        "    state = State()\n",
        "\n",
        "    # 게임 종료 시까지 반복\n",
        "    while True:\n",
        "        # 게임 종료 시\n",
        "        if state.is_done():\n",
        "            break\n",
        "\n",
        "        # 행동 얻기\n",
        "        next_action = next_actions[0] if state.is_first_player() else next_actions[1]\n",
        "        action = next_action(state)\n",
        "\n",
        "        # 다음 상태 얻기\n",
        "        state = state.next(action)\n",
        "\n",
        "    # 선 수 플레이어의 포인트 반환\n",
        "    return first_player_point(state)\n",
        "\n",
        "\n",
        "# 베스트 플레이어 교대\n",
        "def update_best_player():\n",
        "    copy('./model/latest.h5', './model/best.h5')\n",
        "    print('Change BestPlayer')\n",
        "\n",
        "\n",
        "# 네트워크 평가\n",
        "def evaluate_network():\n",
        "    # 최신 플레이어 모델 로드\n",
        "    model0 = torch.load('./model/latest.pth', map_location=device, weights_only=False)\n",
        "    # model0.eval()  # 평가 모드로 설정\n",
        "\n",
        "    # 베스트 플레이어 모델 로드\n",
        "    model1 = torch.load('./model/best.pth', map_location=device, weights_only=False)\n",
        "    # model1.eval()  # 평가 모드로 설정\n",
        "\n",
        "    # PV MCTS를 활용해 행동 선택을 수행하는 함수 생성\n",
        "    next_action0 = pv_mcts_action(model0, EN_TEMPERATURE)\n",
        "    next_action1 = pv_mcts_action(model1, EN_TEMPERATURE)\n",
        "    next_actions = (next_action0, next_action1)\n",
        "\n",
        "    # 여러 차례 대전을 반복\n",
        "    total_point = 0\n",
        "    for i in range(EN_GAME_COUNT):\n",
        "        # 1 게임 실행\n",
        "        if i % 2 == 0:\n",
        "            total_point += play(next_actions)\n",
        "        else:\n",
        "            total_point += 1 - play(list(reversed(next_actions)))\n",
        "\n",
        "        # 출력\n",
        "        print('\\rEvaluate {}/{}'.format(i + 1, EN_GAME_COUNT), end='')\n",
        "    print('')\n",
        "\n",
        "    # 평균 포인트 계산\n",
        "    average_point = total_point / EN_GAME_COUNT\n",
        "    print('AveragePoint', average_point)\n",
        "\n",
        "    # 모델 파기\n",
        "    del model0\n",
        "    del model1\n",
        "\n",
        "    # 베스트 플레이어 교대\n",
        "    if average_point > 0.5:\n",
        "        update_best_player()\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    evaluate_network()"
      ],
      "metadata": {
        "id": "fRjkU6oYrW0r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "outputId": "37c0086d-0e49-4698-99d9-6341dc87e7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'collections.OrderedDict' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-155c6e5ed023>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# 동작 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-155c6e5ed023>\u001b[0m in \u001b[0;36mevaluate_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# 1 게임 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mtotal_point\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtotal_point\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-155c6e5ed023>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(next_actions)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# 행동 얻기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_first_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# 다음 상태 얻기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b34977230ecb>\u001b[0m in \u001b[0;36mpv_mcts_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpv_mcts_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpv_mcts_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpv_mcts_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b34977230ecb>\u001b[0m in \u001b[0;36mpv_mcts_scores\u001b[0;34m(model, state, temperature, device)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# 여러 차례 평가 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPV_EVALUATE_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mroot_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# 합법적인 수의 확률 분포\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b34977230ecb>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# 뉴럴 네트워크 추론을 활용한 정책과 가치 얻기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mpolicies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# 누계 가치와 시행 횟수 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b34977230ecb>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, state, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# 추론\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 정책 얻기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ACY4fx3DnHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}