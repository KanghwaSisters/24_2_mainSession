{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgovgQn7CCqJ"
      },
      "source": [
        "##**<AlphaZero를 활용한 틱택토 구현>**\n",
        "####1. 게임 환경(TicTacToe 클래스)을 강화 학습 알고리즘에 통합\n",
        "####2. MCTS(몬테카를로 트리 서치)\n",
        "####3. 신경망(듀엘 네트워크)\n",
        "####4. 자가 대국(Self-Play)을 포함하는 구조를 설계\n",
        "####5. 학습 루프(train 코드)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIHrYSH41dik"
      },
      "source": [
        "#### **1. 게임 환경(TicTacToe 클래스)을 강화 학습 알고리즘에 통합**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "i5a8c9Yp0wvt"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [' '] * 9 # 9개 요소로 이루어진 리스트\n",
        "        self.current_winner = None  # 승자가 있는 경우 승자('X' 또는 'O')를 저장\n",
        "\n",
        "    def available_moves(self): # 사용 가능한 위치(보드의 빈 공간)의 목록을 반환\n",
        "        return [i for i, spot in enumerate(self.board) if spot == ' ']\n",
        "        # 사용 가능한 인덱스 목록 반환\n",
        "\n",
        "    def empty_squares(self):\n",
        "        return ' ' in self.board\n",
        "\n",
        "    def num_empty_squares(self):\n",
        "        return self.board.count(' ') # 보드의 빈 공간 개수 반환\n",
        "\n",
        "    def make_move(self, square, letter):\n",
        "        if self.board[square] == ' ':\n",
        "            self.board[square] = letter # 'X' 또는 'O'를 저장\n",
        "            if self.winner(square, letter):\n",
        "                self.current_winner = letter\n",
        "            return True # 플레이어가 이번 move로 인해 이겼다면 True 출력\n",
        "        return False\n",
        "\n",
        "    def winner(self, square, letter): # 플레이어가 이번 move로 인해 이겼는지 확인\n",
        "        # 이기는 조건 (행, 열, 또는 대각선으로 3개)\n",
        "        win_conditions = [\n",
        "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # 행\n",
        "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # 열\n",
        "            [0, 4, 8], [2, 4, 6]              # 대각선\n",
        "        ]\n",
        "        for condition in win_conditions:\n",
        "            if all(self.board[i] == letter for i in condition): # 플레이어가 승리 조건 만족하는지 확인\n",
        "                return True # 이겼다면 True 출력\n",
        "        return False\n",
        "\n",
        "    # AlphaZero에서 상태가 숫자 배열로 처리됨!\n",
        "    # self.board를 numpy 배열로 변환하는 함수 추가\n",
        "    def get_state(self):\n",
        "      state = np.array([1 if x == 'X' else -1 if x == 'O' else 0 for x in self.board])\n",
        "      return state.reshape(3, 3)  # 3x3 배열 반환\n",
        "\n",
        "    # 게임 종료 시 1, -1, 또는 0의 보상을 반환\n",
        "    def reward(self, letter):\n",
        "      if self.current_winner == letter:\n",
        "        return 1  # 승리 보상\n",
        "      elif self.current_winner is None and not self.empty_squares():\n",
        "        return 0  # 무승부\n",
        "      else:\n",
        "        return -1  # 패배 보상\n",
        "\n",
        "    def print_board(self): # 보드 시각화\n",
        "        for row in [self.board[i * 3:(i + 1) * 3] for i in range(3)]:\n",
        "            print('| ' + ' | '.join(row) + ' |')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbQIOdL6C160"
      },
      "source": [
        "####**2. MCTS(몬테카를로 트리 서치)**\n",
        "#####(1) 노드 정의: 상태(state), 방문 횟수(N), 누적 보상(W), 확률(Prior Probability)을 포함\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "bTGs4UmcJyjb"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, state, parent=None, prior=0):\n",
        "        self.state = state  # 현재 상태\n",
        "        self.parent = parent  # 부모 노드\n",
        "        self.children = {}  # 자식 노드 (행동: 노드)\n",
        "        self.visits = 0  # 방문 횟수\n",
        "        self.value_sum = 0  # 누적 가치\n",
        "        self.prior = prior  # 정책 확률/ 신경망이 예측한 행동의 확률 값 (신경망 출력)\n",
        "\n",
        "    def get_value(self, epsilon=1.0):\n",
        "        ucb = epsilon * self.prior * np.sqrt(self.parent.visits) / (1 + self.visits) # 값을 계산하여 자식 노드 선택 시 사용\n",
        "        return self.value_sum / (1 + self.visits) + ucb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff37dmsAFeKc"
      },
      "source": [
        "#####(2) 탐색/ 확장/ 시뮬레이션/ 역전파"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "JMEzaX2iH83o"
      },
      "outputs": [],
      "source": [
        "# mcts_search 함수: 현재 상태에서 MCTS를 실행하여 행동 확률 분포를 반환\n",
        "def mcts_search(game, model, simulations):\n",
        "    \"\"\"\n",
        "    MCTS를 실행하여 행동 확률 분포를 생성합니다.\n",
        "    :param game: 현재 TicTacToe 게임 인스턴스\n",
        "    :param model: 신경망 모델\n",
        "    :param simulations: MCTS 시뮬레이션 횟수\n",
        "    :return: 행동 확률 분포 (크기: 9)\n",
        "    \"\"\"\n",
        "    root = Node(state=game.get_state())  # 현재 상태를 루트 노드로 설정\n",
        "\n",
        "    for _ in range(simulations):\n",
        "        node = root\n",
        "        temp_game = TicTacToe()  # MCTS 탐색을 위해 게임을 복사\n",
        "        temp_game.board = game.board.copy()\n",
        "\n",
        "        # 1. 탐색 (Select)\n",
        "        while node.children:\n",
        "            action, node = max(node.children.items(), key=lambda item: item[1].get_value())\n",
        "\n",
        "            temp_game.make_move(action, 'X')  # 선택한 행동을 임시 게임에 반영\n",
        "\n",
        "        # 2. 확장 (Expand)\n",
        "        if temp_game.empty_squares():\n",
        "            legal_moves = temp_game.available_moves()\n",
        "            policy, value = model.predict(temp_game.get_state().reshape(1, 3, 3))\n",
        "\n",
        "            for move in legal_moves:\n",
        "                if move not in node.children:\n",
        "                    next_state = simulate_move(node.state, move)\n",
        "                    node.children[move] = Node(state=next_state, parent=node, prior=policy[0][move])\n",
        "\n",
        "            # 확장된 노드 중 하나를 선택\n",
        "            action, node = max(node.children.items(), key=lambda item: item[1].prior)\n",
        "\n",
        "            temp_game.make_move(action, 'X')\n",
        "\n",
        "        # 3. 시뮬레이션 (Simulate)\n",
        "        reward = temp_game.reward('X') if temp_game.current_winner else value[0][0]\n",
        "\n",
        "        # 4. 역전파 (Backpropagate)\n",
        "        backpropagate(node, reward)\n",
        "\n",
        "    # 행동 확률 계산\n",
        "    actions = np.zeros(9)  # 행동 확률 분포 (틱택토에서는 9개의 행동)\n",
        "    for move, child in root.children.items():\n",
        "        actions[move] = child.visits\n",
        "    actions /= sum(actions)  # 확률로 정규화\n",
        "\n",
        "    return actions # 행동 확률 분포 반환 (크기: 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "77-ALRurIAHz"
      },
      "outputs": [],
      "source": [
        "# 보조 함수: MCTS에서 특정 행동을 수행한 후의 상태를 반환\n",
        "def simulate_move(state, move):\n",
        "    new_state = state.copy()\n",
        "    row, col = divmod(move, 3)\n",
        "    new_state[row, col] = 1  # 플레이어의 행동\n",
        "    return new_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRyyFVYvC2oe"
      },
      "source": [
        "####**3. 신경망(듀엘 네트워크)**\n",
        "##### AlphaZero는 정책(Policy)과 값(Value)을 동시에 출력하는 신경망을 사용!\n",
        "#####입력: 3x3 보드 상태\n",
        "#####출력: 정책(Policy): 각 행동의 확률 (크기: 9) & 값(Value): 현재 상태의 예상 승률 (크기: 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TicTcToeModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TicTcToeModel, self).__init__()\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "        self.fc1 = nn.Linear(9, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.policy_head = nn.Linear(64, 9) # 정책 출력 계층\n",
        "        self.value_head = nn.Linear(64, 1) # 가치 출력 계층\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1) # (batch_size, 3, 3) -> (batch_size, 9)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        policy_output = F.softmax(self.policy_head(x), dim=1)  # 정책 출력 with softmax\n",
        "        value_output = torch.tanh(self.value_head(x))          # 가치 출력 with tanh\n",
        "        return policy_output, value_output\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"배치 입력에 대해 정책 및 가치 예측.\"\"\"\n",
        "        self.eval()  # 모델을 평가 모드로 전환\n",
        "        with torch.no_grad():\n",
        "            policy, value = self.forward(x)\n",
        "        return policy, value\n",
        "\n",
        "# 모델\n",
        "model = TicTcToeModel()\n",
        "\n",
        "# 옵티마이저 및 손실 정의\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion_policy = nn.CrossEntropyLoss()\n",
        "criterion_value = nn.MSELoss()\n",
        "\n",
        "# 손실 계산 예시\n",
        "dummy_input = torch.rand(4, 3, 3)  # 배치 사이즈 4, 3x3 인\n",
        "policy_target = torch.randint(0, 9, (4,))  # 정책 더미\n",
        "value_target = torch.rand(4, 1)  # 가치 더미\n",
        "\n",
        "# Forward pass\n",
        "policy_output, value_output = model(dummy_input)\n",
        "\n",
        "# 손실 계산\n",
        "policy_loss = criterion_policy(policy_output, policy_target)\n",
        "value_loss = criterion_value(value_output, value_target)\n",
        "total_loss = policy_loss + value_loss\n",
        "\n",
        "# 역전파\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "ZX2UsKNKLViP"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-NYrc7KC2-1"
      },
      "source": [
        "####**4. 자가 대국(Self-Play)을 포함하는 구조를 설계**\n",
        "#####AlphaZero는 스스로 대국을 반복하며 데이터를 생성하고, 이를 통해 신경망을 학습함!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "W9cxRfR1DBAN"
      },
      "outputs": [],
      "source": [
        "def self_play(game, model, mcts_simulations=100):\n",
        "    data = []\n",
        "\n",
        "    while game.empty_squares():\n",
        "        # MCTS 실행\n",
        "        action_probs = mcts_search(game, model, mcts_simulations)\n",
        "        action = np.random.choice(len(action_probs), p=action_probs)\n",
        "\n",
        "        # 데이터 저장\n",
        "        state = game.get_state()\n",
        "        data.append((state, action_probs, None))  # 보상은 게임 종료 후 추가\n",
        "\n",
        "        # 행동 수행\n",
        "        game.make_move(action, 'X')\n",
        "        if game.current_winner or not game.empty_squares():\n",
        "            break\n",
        "\n",
        "    # 게임 종료 후 보상 업데이트\n",
        "    reward = game.reward('X')\n",
        "    for i in range(len(data)):\n",
        "        data[i] = (data[i][0], data[i][1], reward)\n",
        "        reward = -reward  # 상대방 보상 반전\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9zmf2YwF8Vo"
      },
      "source": [
        "#### **5. 학습 루프(train 코드)**\n",
        "#####반복적으로 Self-Play를 통해 데이터를 생성하고, 신경망을 업데이트~"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def train(model, game_class, self_play_fn, optimizer, criterion_policy, criterion_value,\n",
        "          iterations=10, self_play_games=50, batch_size=32, mcts_simulations=25):\n",
        "    \"\"\"\n",
        "    알파제로 기반 틱택토 학습 루프.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): TicTcToeModel 모델.\n",
        "        game_class (class): 틱택토 게임 클래스.\n",
        "        self_play_fn (func): Self-play 함수.\n",
        "        optimizer (torch.optim.Optimizer): PyTorch 옵티마이저.\n",
        "        criterion_policy (nn.Module): 정책 손실 함수.\n",
        "        criterion_value (nn.Module): 가치 손실 함수.\n",
        "        iterations (int): 학습 반복 횟수.\n",
        "        self_play_games (int): Self-play로 생성할 게임 수.\n",
        "        batch_size (int): 학습 배치 크기.\n",
        "        mcts_simulations (int): MCTS 시뮬레이션 횟수.\n",
        "    \"\"\"\n",
        "    for iteration in range(1, iterations + 1):\n",
        "        print(f\"=== Iteration {iteration}/{iterations} ===\")\n",
        "\n",
        "        # Step 1: Self-play로 데이터 생성\n",
        "        all_data = []\n",
        "        for _ in range(self_play_games):\n",
        "            game = game_class()\n",
        "            data = self_play_fn(game, model, mcts_simulations)\n",
        "            all_data.extend(data)\n",
        "\n",
        "        # Step 2: 데이터 준비\n",
        "        states, policies, rewards = zip(*all_data)\n",
        "        states = torch.from_numpy(np.array(states)).float()  # (batch_size, 3, 3)\n",
        "        policies = torch.tensor(policies, dtype=torch.float32)  # (batch_size, 9)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "        # Step 3: DataLoader로 배치 처리\n",
        "        dataset = TensorDataset(states, policies, rewards)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Step 4: 모델 학습\n",
        "        model.train()\n",
        "        total_policy_loss, total_value_loss = 0, 0\n",
        "\n",
        "        for batch_states, batch_policies, batch_rewards in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            policy_output, value_output = model(batch_states)\n",
        "\n",
        "            # 손실 계산\n",
        "            policy_loss = criterion_policy(policy_output, batch_policies)\n",
        "            value_loss = criterion_value(value_output, batch_rewards)\n",
        "            loss = policy_loss + value_loss\n",
        "\n",
        "            # 역전파 및 옵티마이저 스텝\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 손실 기록\n",
        "            total_policy_loss += policy_loss.item()\n",
        "            total_value_loss += value_loss.item()\n",
        "\n",
        "        # 평균 손실 출력\n",
        "        avg_policy_loss = total_policy_loss / len(dataloader)\n",
        "        avg_value_loss = total_value_loss / len(dataloader)\n",
        "        print(f\"Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "1Jo7IL4sY3gV"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델, 옵티마이저, 손실 정의\n",
        "model = TicTcToeModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion_policy = nn.MSELoss()  # 정책 출력에 맞게 설정\n",
        "criterion_value = nn.MSELoss()\n",
        "\n",
        "# 학습 실행\n",
        "train(\n",
        "    model=model,\n",
        "    game_class=TicTacToe,  # TicTacToe 게임 클래스 전달\n",
        "    self_play_fn=self_play,  # Self-play 함수 전달\n",
        "    optimizer=optimizer,\n",
        "    criterion_policy=criterion_policy,\n",
        "    criterion_value=criterion_value,\n",
        "    iterations=10,  # 학습 반복 횟수\n",
        "    self_play_games=50,  # Self-play로 생성할 게임 수\n",
        "    batch_size=32,  # 학습 배치 크기\n",
        "    mcts_simulations=25  # MCTS 시뮬레이션 횟수\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qKityKajW2-l",
        "outputId": "f2c8b7bd-f889-4c57-842d-b7ba8792fef9"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Iteration 1/10 ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "flatten(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-283-3ada4822607b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 학습 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgame_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTicTacToe\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TicTacToe 게임 클래스 전달\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-282-10d179ccca7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, game_class, self_play_fn, optimizer, criterion_policy, criterion_value, iterations, self_play_games, batch_size, mcts_simulations)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_play_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_play_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-281-46cf9cd3a111>\u001b[0m in \u001b[0;36mself_play\u001b[0;34m(game, model, mcts_simulations)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# MCTS 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-278-723660a155c8>\u001b[0m in \u001b[0;36mmcts_search\u001b[0;34m(game, model, simulations)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemp_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlegal_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-280-daa397481c47>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 모델을 평가 모드로 전환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-280-daa397481c47>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, 3, 3) -> (batch_size, 9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: flatten(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}