{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbotiJ3KbtWf"
      },
      "source": [
        "# AlphaZero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbyMHzrLp5P0"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "# 선공:  1, 'O' #\n",
        "# 후공: -1, 'X' #\n",
        "#################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXbCtaitQ_Ss"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import namedtuple\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjF-DPiDo7yP"
      },
      "source": [
        "## State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmgUoBNBCDNi"
      },
      "outputs": [],
      "source": [
        "BOARD_SIZE = (3,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_OHKemkBy7N"
      },
      "outputs": [],
      "source": [
        "class State:\n",
        "    def __init__(self, board_size=BOARD_SIZE, my_actions=None, enemy_actions=None):\n",
        "        self.board_size = board_size # (3,3)\n",
        "        self.num_actions = self.board_size[0] * self.board_size[1] # 3 * 3 = 9\n",
        "        self.action_space = range(self.num_actions)\n",
        "\n",
        "        self.my_actions = [] if my_actions is None else my_actions\n",
        "        self.enemy_actions = [] if enemy_actions is None else enemy_actions\n",
        "\n",
        "        self.board = self.create_board(self.my_actions, self.enemy_actions)\n",
        "\n",
        "        self.available_actions = self.get_available_actions()\n",
        "\n",
        "    def next(self, action):\n",
        "        '''\n",
        "        내 행동 이후 상대방 턴으로 변경\n",
        "        '''\n",
        "        my_actions = self.my_actions.copy()\n",
        "        my_actions.append(action)\n",
        "\n",
        "        return State(self.board_size, self.enemy_actions, my_actions)\n",
        "\n",
        "    def create_board(self, my_actions, enemy_actions):\n",
        "        total_board = np.zeros((2,self.board_size[0],self.board_size[1]))\n",
        "\n",
        "        my_board = np.zeros(self.board_size).flatten()\n",
        "        enemy_board = np.zeros(self.board_size).flatten()\n",
        "\n",
        "        my_board[my_actions] = 1\n",
        "        enemy_board[enemy_actions] = 1\n",
        "\n",
        "        total_board[0] = my_board.reshape(self.board_size)\n",
        "        total_board[1] = enemy_board.reshape(self.board_size)\n",
        "\n",
        "        return total_board\n",
        "\n",
        "    def get_available_actions(self):\n",
        "        my_actions_set = set(self.my_actions)\n",
        "        enemy_actions_set = set(self.enemy_actions)\n",
        "\n",
        "        available_actions_set = set(range(self.num_actions)) - my_actions_set - enemy_actions_set\n",
        "\n",
        "        return list(available_actions_set)\n",
        "\n",
        "    def is_win(self):\n",
        "        my_state = self.board[0]\n",
        "\n",
        "        row_win = np.sum(my_state, axis=0).max() == self.board_size[0]\n",
        "        col_win = np.sum(my_state, axis=1).max() == self.board_size[1]\n",
        "        diag_win = np.trace(my_state) == self.board_size[0]\n",
        "        anti_diag_win = np.trace(np.fliplr(my_state)) == self.board_size[0]\n",
        "\n",
        "        return row_win or col_win or diag_win or anti_diag_win\n",
        "\n",
        "    def is_draw(self):\n",
        "        return (np.sum(self.board[0]) + np.sum(self.board[1])) >= self.num_actions\n",
        "\n",
        "    def is_lose(self):\n",
        "        enemy_state = self.board[1]\n",
        "\n",
        "        row_lose = np.sum(enemy_state, axis=0).max() == self.board_size[0]\n",
        "        col_lose = np.sum(enemy_state, axis=1).max() == self.board_size[1]\n",
        "        diag_lose = np.trace(enemy_state) == self.board_size[0]\n",
        "        anti_diag_lose = np.trace(np.fliplr(enemy_state)) == self.board_size[0]\n",
        "\n",
        "        return row_lose or col_lose or diag_lose or anti_diag_lose\n",
        "\n",
        "    def is_done(self):\n",
        "        return self.is_win() or self.is_draw() or self.is_lose()\n",
        "\n",
        "    def is_going_first(self):\n",
        "        return len(self.my_actions) == len(self.enemy_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvECaRBo9s1"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkZ47mFvAAeX"
      },
      "outputs": [],
      "source": [
        "class TicTacToeEnv:\n",
        "    def __init__(self):\n",
        "        self.state = State()\n",
        "\n",
        "        self.reward = {'win': 10, 'lose': -10, 'draw': 0, 'continue': 0}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = State()\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        my_actions = self.state.my_actions.copy()\n",
        "        enemy_actions = self.state.enemy_actions.copy()\n",
        "\n",
        "        my_actions.append(action)\n",
        "\n",
        "        next_state = State(BOARD_SIZE, my_actions, enemy_actions)\n",
        "        self.state = State(BOARD_SIZE, self.state.enemy_actions, my_actions) # 다음 스텝 - 상대방 턴\n",
        "\n",
        "        if next_state.is_win():\n",
        "            reward, done = self.reward['win'], True\n",
        "\n",
        "        elif next_state.is_draw():\n",
        "            reward, done = self.reward['draw'], True\n",
        "\n",
        "        elif next_state.is_lose():\n",
        "            reward, done = self.reward['lose'], True\n",
        "\n",
        "        else:\n",
        "            reward, done = self.reward['continue'], False\n",
        "\n",
        "        return self.state, next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = State()\n",
        "        return self.state\n",
        "\n",
        "    def render(self, state):\n",
        "        board = state.board[0] + (-1 * state.board[1]) if state.is_going_first() else state.board[1] + (-1 * state.board[0])\n",
        "\n",
        "        int_to_symbol = np.where(board == 1, 'O',\n",
        "                                np.where(board == -1, 'X', '-'))\n",
        "\n",
        "        rendering_board = '\\n'.join([' '.join(row) for row in int_to_symbol])\n",
        "\n",
        "        print()\n",
        "        print(rendering_board)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WFnHOTiejov"
      },
      "source": [
        "## Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4uvoFC472mo"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, downsample=False):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj9ckVV477yg"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, config, zero_init_residual=False):\n",
        "        super().__init__()\n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "\n",
        "        # Initial Conv Layer\n",
        "        self.conv1 = nn.Conv2d(2, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Residual Layers\n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1])\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride=1):\n",
        "        layers = []\n",
        "        for i in range(n_blocks):\n",
        "            if i == 0:\n",
        "                layers.append(block(self.in_channels, channels, downsample=(self.in_channels != channels)))\n",
        "            else:\n",
        "                layers.append(block(channels, channels))\n",
        "            self.in_channels = channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.avgpool(x)  # Output shape: (batch_size, channels, 1, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydKX5Gwn06Rq"
      },
      "outputs": [],
      "source": [
        "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVdNBnpQ1ISa"
      },
      "outputs": [],
      "source": [
        "config = ResNetConfig(block=BasicBlock, n_blocks=[2, 2], channels=[64, 64])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw5vkm3D8CiK"
      },
      "outputs": [],
      "source": [
        "class AlphaZeroNet(nn.Module):\n",
        "    def __init__(self, board_size=(3, 3), config=config):\n",
        "        super(AlphaZeroNet, self).__init__()\n",
        "        _, _, channels = config\n",
        "        self.board_size = board_size\n",
        "\n",
        "        self.resnet = ResNet(config=config)\n",
        "\n",
        "        # Policy Head\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Conv2d(channels[-1], 2, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2, board_size[0] * board_size[1]),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Value Head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Conv2d(channels[-1], 1, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)  # Shape: (batch_size, channels, 1, 1)\n",
        "        policy = self.policy_head(x)  # Policy Head\n",
        "        value = self.value_head(x)   # Value Head\n",
        "        return policy, value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QoTSWv0QGel"
      },
      "source": [
        "## MCTS Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fshpebJ6f8Xm"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, state, agent, parent=None, action=None):\n",
        "        self.state = state\n",
        "        self.agent = agent\n",
        "        self.parent_node = parent\n",
        "        self.child_nodes = None\n",
        "        self.action = action  # 현재 노드로 이동할 때의 액션\n",
        "        self.w = 0  # 보상 누계\n",
        "        self.n = 0  # 시행 횟수\n",
        "\n",
        "    def expand(self):\n",
        "        available_actions = self.state.available_actions\n",
        "        self.child_nodes = []\n",
        "        for action in available_actions:\n",
        "            child_state = self.state.next(action)\n",
        "            child_node = Node(child_state, self.agent, parent=self, action=action)  # action 전달\n",
        "            self.child_nodes.append(child_node)\n",
        "\n",
        "    def get_result(self):\n",
        "        if self.state.is_lose():\n",
        "            return -2  # 패배 (상대방 승리) - 값 증가시킴\n",
        "        elif self.state.is_draw():\n",
        "            return 0  # 무승부\n",
        "        else:\n",
        "            return 1  # 승리\n",
        "\n",
        "    # UCB1이 가장 큰 child node 얻기\n",
        "    def next_child_node(self):\n",
        "        # 시행 횟수가 0인 child node 반환\n",
        "        for child_node in self.child_nodes:\n",
        "            if child_node.n == 0:\n",
        "                return child_node\n",
        "\n",
        "        # UCB1 계산\n",
        "        t = 0\n",
        "        for c in self.child_nodes:\n",
        "            t += c.n\n",
        "        ucb1_values = []\n",
        "        for child_node in self.child_nodes:\n",
        "            ucb1_values.append(-child_node.w/child_node.n+(2*math.log(t)/child_node.n)**0.5)\n",
        "\n",
        "        # UCB1이 가장 큰 child node 반환\n",
        "        return self.child_nodes[self.agent.argmax(ucb1_values)]\n",
        "\n",
        "    def backpropagate(self, value):\n",
        "        node = self\n",
        "        while node is not None:\n",
        "            node.w += value\n",
        "            node.n += 1\n",
        "            node = node.parent_node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDhBf-A_pAQs"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUaejoURd1Fm"
      },
      "outputs": [],
      "source": [
        "NUM_OF_SIMULATION = 200\n",
        "TAU_START = 1.0\n",
        "TAU_END = 0.3\n",
        "DECAY_STEPS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9d54yFpe8NX"
      },
      "outputs": [],
      "source": [
        "class AlphaZeroAgent:\n",
        "    def __init__(self, network, env):\n",
        "        self.network = network\n",
        "        self.env = env\n",
        "        self.num_simulations = NUM_OF_SIMULATION\n",
        "        self.tau_start = TAU_START\n",
        "        self.tau_end = TAU_END\n",
        "        self.decay_steps = DECAY_STEPS\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_temperature(self):\n",
        "        tau = max(self.tau_end, self.tau_start - (self.tau_start - self.tau_end) * (self.current_step / self.decay_steps))\n",
        "\n",
        "        return tau\n",
        "\n",
        "    def run_mcts(self, root_state):\n",
        "        root = Node(root_state, agent=self)\n",
        "\n",
        "        if not root.child_nodes:  # 자식 노드가 없는 경우\n",
        "            root.expand()\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            leaf_node, value = self.simulate(root)\n",
        "            leaf_node.backpropagate(value)\n",
        "\n",
        "        # 방문 횟수를 기반으로 정책 계산\n",
        "        policy = np.zeros(root_state.num_actions, dtype=np.float32)\n",
        "        for child in root.child_nodes:\n",
        "            policy[child.action] = child.n\n",
        "\n",
        "        tau = self.get_temperature()\n",
        "        policy = np.clip(policy, 1e-4, None)  # 너무 작은 값을 클립\n",
        "        policy = policy ** (1 / tau)\n",
        "        if np.sum(policy) > 0:\n",
        "            policy /= np.sum(policy)  # 정규화\n",
        "        else:\n",
        "            policy = np.ones_like(policy) / len(policy)  # 모든 액션에 균등한 확률 할당\n",
        "\n",
        "\n",
        "        self.current_step += 1  # 학습 진행 업데이트\n",
        "        # policy 계산 후 확인\n",
        "        # if np.sum(policy) != 1:\n",
        "        #     print(f\"Warning: Sum of policy probabilities is not 1. Actual sum: {np.sum(policy)}\")\n",
        "        # print(f\"MCTS policy : {policy}\\n\")\n",
        "\n",
        "        return policy\n",
        "\n",
        "    def simulate(self, node):\n",
        "        if node.state.is_done():\n",
        "            return node, node.get_result()\n",
        "\n",
        "        # 즉각적인 상대방의 승리를 방지\n",
        "        if node.state.is_lose():\n",
        "            return node, -1  # 최악의 결과 방지\n",
        "\n",
        "        if node.child_nodes is None:\n",
        "            node.expand()\n",
        "            return node, self.predict_value(node.state)\n",
        "\n",
        "        best_child = node.next_child_node()\n",
        "        return self.simulate(best_child)\n",
        "\n",
        "\n",
        "    def predict_value(self, state):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(state.board, dtype=torch.float32).unsqueeze(0)\n",
        "            _, value = self.network(state_tensor)\n",
        "        # print(f\"MCTS value : {value.item()}\")\n",
        "        return value.item()\n",
        "\n",
        "    def playout(self, state):\n",
        "        if state.is_lose():\n",
        "            return self.env.reward['lose']\n",
        "\n",
        "        if state.is_draw():\n",
        "            return  self.env.reward['draw']\n",
        "\n",
        "        # 다음 상태의 상태 평가\n",
        "        return -self.playout(state.next(self.random_action(state)))\n",
        "\n",
        "    def random_action(self, state):\n",
        "        available_actions = state.available_actions\n",
        "        return np.random.choice(available_actions)\n",
        "\n",
        "    def argmax(self, collection):\n",
        "        max_idx_list = np.arange(len(collection))[collection == np.max(collection)]\n",
        "        return np.random.choice(max_idx_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Nyf43dgpHv"
      },
      "source": [
        "## Self-play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARAI_eckZj2O"
      },
      "outputs": [],
      "source": [
        "def self_play(env, agent):\n",
        "    states, mcts_policies, values = [], [], []\n",
        "    state = env.reset()\n",
        "\n",
        "    while not state.is_done():\n",
        "        policy = agent.run_mcts(state)\n",
        "        action = np.random.choice(len(policy), p=policy)\n",
        "        states.append(state.board)\n",
        "        mcts_policies.append(policy)\n",
        "        state, _, _, _ = env.step(action)\n",
        "\n",
        "    if state.is_win():\n",
        "        outcome = 1\n",
        "    elif state.is_draw():\n",
        "        outcome = 0\n",
        "    else:\n",
        "        outcome = -1\n",
        "\n",
        "    for _ in range(len(states)):\n",
        "        values.append(outcome)\n",
        "        outcome = -outcome  # 상대방의 결과는 반대가 됨\n",
        "\n",
        "    return states, mcts_policies, values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_89tSXZgvbm"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xat5X0jhgwoZ"
      },
      "outputs": [],
      "source": [
        "def train(network, optimizer, data):\n",
        "    network.train()\n",
        "    total_loss = 0\n",
        "    for states, policies, values in data:\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "        policies = torch.tensor(np.array(policies), dtype=torch.float32)\n",
        "        values = torch.tensor(np.array(values), dtype=torch.float32)\n",
        "\n",
        "        pred_policies, pred_values = network(states)\n",
        "\n",
        "        # 상대방 위협 방지 페널티 추가\n",
        "        pred_policies = torch.softmax(pred_policies, dim=-1)\n",
        "        penalty = (policies * (pred_policies < 0.1).float()).sum(dim=1)\n",
        "        policy_loss = -torch.sum(policies * torch.log(pred_policies + 1e-6)).mean() + penalty.mean()\n",
        "        value_loss = ((values - pred_values.view(-1)) ** 2).mean()\n",
        "\n",
        "        loss = policy_loss + value_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9H8AjyTZVv1"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKCuyncvwAZM"
      },
      "outputs": [],
      "source": [
        "def evaluate_network(best_net, current_net, env, num_games=50):\n",
        "    current_agent = AlphaZeroAgent(current_net, env)\n",
        "    best_agent = AlphaZeroAgent(best_net, env)\n",
        "\n",
        "    current_net_wins, best_net_wins, draws = 0, 0, 0\n",
        "\n",
        "    for i in range(num_games):\n",
        "        state = env.reset()\n",
        "\n",
        "        while not state.is_done():\n",
        "            if state.is_going_first():\n",
        "                # 현재 네트워크의 행동\n",
        "                policy = current_agent.run_mcts(state)\n",
        "            else:\n",
        "                # 이전 베스트 네트워크의 행동\n",
        "                policy = best_agent.run_mcts(state)\n",
        "\n",
        "            action = np.random.choice(len(policy), p=policy)\n",
        "            state, _, _, _ = env.step(action)\n",
        "\n",
        "        # 게임 결과 확인\n",
        "        if state.is_win():\n",
        "            if state.is_going_first():\n",
        "                current_net_wins += 1\n",
        "            else:\n",
        "                best_net_wins += 1\n",
        "        elif state.is_draw():\n",
        "            draws += 1\n",
        "        else:\n",
        "            if state.is_going_first():\n",
        "                best_net_wins += 1\n",
        "            else:\n",
        "                current_net_wins += 1\n",
        "\n",
        "    print(f\"Evaluation: Current Net Wins: {current_net_wins}, Best Net Wins: {best_net_wins}, Draws: {draws}\")\n",
        "    return current_net_wins, best_net_wins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xeu86H1ypHlx"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N84y_FfRNUg",
        "outputId": "4e6a28bf-4242-46e6-81b4-b5d391c07b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, file_path):\n",
        "    torch.save({\n",
        "        'model_state': model.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }, file_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch} to {file_path}\")"
      ],
      "metadata": {
        "id": "_mkVCeSZOHjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        checkpoint = torch.load(file_path)\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "        start_epoch = checkpoint['epoch'] + 1  # 다음 에포크부터 시작\n",
        "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        print(\"No checkpoint found. Starting from scratch.\")\n",
        "    return start_epoch"
      ],
      "metadata": {
        "id": "dia344AVOJxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TicTacToeEnv()\n",
        "net = AlphaZeroNet()\n",
        "agent = AlphaZeroAgent(net, env)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
        "\n",
        "best_net = AlphaZeroNet()\n",
        "best_net.load_state_dict(net.state_dict())\n",
        "\n",
        "# 체크포인트 로드\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/kanghwa'\n",
        "file_path = os.path.join(folder_path, 'checkpoint2.pth')\n",
        "start_epoch = load_checkpoint(net, optimizer, file_path)\n",
        "\n",
        "num_epochs = 16\n",
        "num_games = 100\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    data = [self_play(env, agent) for _ in range(num_games)]\n",
        "    loss = train(net, optimizer, data)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss}\")\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        current_net_wins, best_net_wins = evaluate_network(best_net, net, env, num_games)\n",
        "\n",
        "        if current_net_wins > best_net_wins:\n",
        "            print(\"Update best network.\")\n",
        "            best_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    save_checkpoint(best_net, optimizer, epoch, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e068Ji8bOKcn",
        "outputId": "fcb269f0-ba0b-4109-ee20-a6a6f7259e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found. Starting from scratch.\n",
            "Epoch 1/16, Loss: 12.014530134201049\n",
            "Checkpoint saved at epoch 0 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 2/16, Loss: 11.67139497756958\n",
            "Evaluation: Current Net Wins: 98, Best Net Wins: 2, Draws: 0\n",
            "Update best network.\n",
            "Checkpoint saved at epoch 1 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 3/16, Loss: 11.357373151779175\n",
            "Checkpoint saved at epoch 2 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 4/16, Loss: 11.41596399307251\n",
            "Evaluation: Current Net Wins: 98, Best Net Wins: 2, Draws: 0\n",
            "Update best network.\n",
            "Checkpoint saved at epoch 3 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 5/16, Loss: 11.40026351928711\n",
            "Checkpoint saved at epoch 4 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 6/16, Loss: 11.306687183380127\n",
            "Evaluation: Current Net Wins: 94, Best Net Wins: 6, Draws: 0\n",
            "Update best network.\n",
            "Checkpoint saved at epoch 5 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 7/16, Loss: 11.28530813217163\n",
            "Checkpoint saved at epoch 6 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 8/16, Loss: 11.176020555496216\n",
            "Evaluation: Current Net Wins: 94, Best Net Wins: 6, Draws: 0\n",
            "Update best network.\n",
            "Checkpoint saved at epoch 7 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 9/16, Loss: 10.958401165008546\n",
            "Checkpoint saved at epoch 8 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 10/16, Loss: 11.140824489593506\n",
            "Evaluation: Current Net Wins: 95, Best Net Wins: 5, Draws: 0\n",
            "Update best network.\n",
            "Checkpoint saved at epoch 9 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 11/16, Loss: 11.060823278427124\n",
            "Checkpoint saved at epoch 10 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 12/16, Loss: 10.782007904052735\n",
            "Evaluation: Current Net Wins: 96, Best Net Wins: 4, Draws: 0\n",
            "Update best network.\n",
            "Checkpoint saved at epoch 11 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 13/16, Loss: 10.994355392456054\n",
            "Checkpoint saved at epoch 12 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 14/16, Loss: 10.813211326599122\n",
            "Evaluation: Current Net Wins: 95, Best Net Wins: 5, Draws: 0\n",
            "Update best network.\n",
            "Checkpoint saved at epoch 13 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 15/16, Loss: 10.642253112792968\n",
            "Checkpoint saved at epoch 14 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n",
            "Epoch 16/16, Loss: 10.73874093055725\n",
            "Evaluation: Current Net Wins: 96, Best Net Wins: 4, Draws: 0\n",
            "Update best network.\n",
            "Checkpoint saved at epoch 15 to /content/drive/MyDrive/Colab Notebooks/kanghwa/checkpoint2.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "y7iLsFmIOizJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcHfCQm_Cgch"
      },
      "outputs": [],
      "source": [
        "def test_agent(agent, env, num_games=10, human_play=False):\n",
        "    agent_wins = 0\n",
        "    draws = 0\n",
        "    losses = 0\n",
        "\n",
        "    for game in range(num_games):\n",
        "        print(f\"Game {game + 1}/{num_games}\")\n",
        "        state = env.reset()\n",
        "        env.render(state)\n",
        "\n",
        "        while not state.is_done():\n",
        "            if state.is_going_first():  # Agent's turn\n",
        "                policy = agent.run_mcts(state)\n",
        "                action = np.random.choice(len(policy), p=policy)\n",
        "            else:  # Opponent's turn\n",
        "                if human_play:\n",
        "                    # 사람의 입력을 받아서 동작\n",
        "                    action = int(input(\"Enter your move (0-8): \"))\n",
        "                    while action not in state.available_actions:\n",
        "                        action = int(input(\"Invalid move. Try again (0-8): \"))\n",
        "                else:\n",
        "                    # 랜덤한 상대\n",
        "                    action = np.random.choice(state.available_actions)\n",
        "\n",
        "            state, _, _, _ = env.step(action)\n",
        "            env.render(state)\n",
        "\n",
        "        # 게임 결과가 끝난 후 처리\n",
        "        if state.is_lose():\n",
        "            if state.is_going_first():\n",
        "                print(\"Opponent wins!\")\n",
        "                losses += 1\n",
        "            else:\n",
        "                print(\"Agent wins!\")\n",
        "                agent_wins += 1\n",
        "        elif state.is_draw():\n",
        "            print(\"It's a draw!\")\n",
        "            draws += 1\n",
        "\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(f\"Agent Wins: {agent_wins}\")\n",
        "    print(f\"Draws: {draws}\")\n",
        "    print(f\"Losses: {losses}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6SkUdu9-mJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee80842-3253-45c3-b48c-854ab820449b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Game 1/5\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - O\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 4\n",
            "\n",
            "- - O\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - O\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 1\n",
            "\n",
            "O X O\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "\n",
            "O X O\n",
            "- X O\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 8\n",
            "\n",
            "O X O\n",
            "- X O\n",
            "- - X\n",
            "\n",
            "\n",
            "O X O\n",
            "O X O\n",
            "- - X\n",
            "\n",
            "Enter your move (0-8): 7\n",
            "\n",
            "O X O\n",
            "O X O\n",
            "- X X\n",
            "\n",
            "Opponent wins!\n",
            "Game 2/5\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - O\n",
            "\n",
            "Enter your move (0-8): 4\n",
            "\n",
            "- - -\n",
            "- X -\n",
            "- - O\n",
            "\n",
            "\n",
            "- - -\n",
            "- X O\n",
            "- - O\n",
            "\n",
            "Enter your move (0-8): 2\n",
            "\n",
            "- - X\n",
            "- X O\n",
            "- - O\n",
            "\n",
            "\n",
            "- - X\n",
            "O X O\n",
            "- - O\n",
            "\n",
            "Enter your move (0-8): 6\n",
            "\n",
            "- - X\n",
            "O X O\n",
            "X - O\n",
            "\n",
            "Opponent wins!\n",
            "Game 3/5\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - O\n",
            "\n",
            "Enter your move (0-8): 4\n",
            "\n",
            "- - -\n",
            "- X -\n",
            "- - O\n",
            "\n",
            "\n",
            "- - -\n",
            "- X O\n",
            "- - O\n",
            "\n",
            "Enter your move (0-8): 2\n",
            "\n",
            "- - X\n",
            "- X O\n",
            "- - O\n",
            "\n",
            "\n",
            "- - X\n",
            "O X O\n",
            "- - O\n",
            "\n",
            "Enter your move (0-8): 6\n",
            "\n",
            "- - X\n",
            "O X O\n",
            "X - O\n",
            "\n",
            "Opponent wins!\n",
            "Game 4/5\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 4\n",
            "\n",
            "O - -\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - O\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 1\n",
            "\n",
            "O X O\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "\n",
            "O X O\n",
            "- X O\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 7\n",
            "\n",
            "O X O\n",
            "- X O\n",
            "- X -\n",
            "\n",
            "Opponent wins!\n",
            "Game 5/5\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - O\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 4\n",
            "\n",
            "- - O\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - O\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 1\n",
            "\n",
            "O X O\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "\n",
            "O X O\n",
            "- X O\n",
            "- - -\n",
            "\n",
            "Enter your move (0-8): 7\n",
            "\n",
            "O X O\n",
            "- X O\n",
            "- X -\n",
            "\n",
            "Opponent wins!\n",
            "\n",
            "Test Results:\n",
            "Agent Wins: 0\n",
            "Draws: 0\n",
            "Losses: 5\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "agent = AlphaZeroAgent(best_net, env)\n",
        "\n",
        "test_agent(agent, env, num_games=5, human_play=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCmoc0a5K__A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f58acc-014b-46b7-af43-b05533e388d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Game 1/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - -\n",
            "- - X\n",
            "- - -\n",
            "\n",
            "\n",
            "O O -\n",
            "- - X\n",
            "- - -\n",
            "\n",
            "\n",
            "O O -\n",
            "- - X\n",
            "X - -\n",
            "\n",
            "\n",
            "O O O\n",
            "- - X\n",
            "X - -\n",
            "\n",
            "Agent wins!\n",
            "Game 2/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- O -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "X O -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "X O -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "\n",
            "X O -\n",
            "- - -\n",
            "- O X\n",
            "\n",
            "\n",
            "X O -\n",
            "- O -\n",
            "- O X\n",
            "\n",
            "Agent wins!\n",
            "Game 3/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - O\n",
            "\n",
            "\n",
            "- - X\n",
            "- - -\n",
            "- - O\n",
            "\n",
            "\n",
            "O - X\n",
            "- - -\n",
            "- - O\n",
            "\n",
            "\n",
            "O - X\n",
            "- - -\n",
            "- X O\n",
            "\n",
            "\n",
            "O - X\n",
            "- O -\n",
            "- X O\n",
            "\n",
            "Agent wins!\n",
            "Game 4/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - -\n",
            "X - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O O -\n",
            "X - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O O -\n",
            "X - -\n",
            "- - X\n",
            "\n",
            "\n",
            "O O O\n",
            "X - -\n",
            "- - X\n",
            "\n",
            "Agent wins!\n",
            "Game 5/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- O -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- O X\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- O X\n",
            "- - O\n",
            "- - -\n",
            "\n",
            "\n",
            "- O X\n",
            "- X O\n",
            "- - -\n",
            "\n",
            "\n",
            "O O X\n",
            "- X O\n",
            "- - -\n",
            "\n",
            "\n",
            "O O X\n",
            "X X O\n",
            "- - -\n",
            "\n",
            "\n",
            "O O X\n",
            "X X O\n",
            "- O -\n",
            "\n",
            "\n",
            "O O X\n",
            "X X O\n",
            "- O X\n",
            "\n",
            "\n",
            "O O X\n",
            "X X O\n",
            "O O X\n",
            "\n",
            "It's a draw!\n",
            "Game 6/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - -\n",
            "O - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - -\n",
            "O - -\n",
            "- - X\n",
            "\n",
            "\n",
            "- - -\n",
            "O - -\n",
            "O - X\n",
            "\n",
            "\n",
            "- - -\n",
            "O - -\n",
            "O X X\n",
            "\n",
            "\n",
            "O - -\n",
            "O - -\n",
            "O X X\n",
            "\n",
            "Agent wins!\n",
            "Game 7/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - -\n",
            "- - -\n",
            "X - -\n",
            "\n",
            "\n",
            "O - O\n",
            "- - -\n",
            "X - -\n",
            "\n",
            "\n",
            "O - O\n",
            "- - -\n",
            "X - X\n",
            "\n",
            "\n",
            "O O O\n",
            "- - -\n",
            "X - X\n",
            "\n",
            "Agent wins!\n",
            "Game 8/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - -\n",
            "O - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - -\n",
            "O - -\n",
            "- X -\n",
            "\n",
            "\n",
            "- - -\n",
            "O - -\n",
            "O X -\n",
            "\n",
            "\n",
            "- - -\n",
            "O X -\n",
            "O X -\n",
            "\n",
            "\n",
            "O - -\n",
            "O X -\n",
            "O X -\n",
            "\n",
            "Agent wins!\n",
            "Game 9/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - O\n",
            "\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- X O\n",
            "\n",
            "\n",
            "- - -\n",
            "- O -\n",
            "- X O\n",
            "\n",
            "\n",
            "- X -\n",
            "- O -\n",
            "- X O\n",
            "\n",
            "\n",
            "O X -\n",
            "- O -\n",
            "- X O\n",
            "\n",
            "Agent wins!\n",
            "Game 10/10\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - X\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "\n",
            "O - X\n",
            "- - -\n",
            "O - -\n",
            "\n",
            "\n",
            "O - X\n",
            "- X -\n",
            "O - -\n",
            "\n",
            "\n",
            "O - X\n",
            "O X -\n",
            "O - -\n",
            "\n",
            "Agent wins!\n",
            "\n",
            "Test Results:\n",
            "Agent Wins: 9\n",
            "Draws: 1\n",
            "Losses: 0\n"
          ]
        }
      ],
      "source": [
        "test_agent(agent, env, num_games=10, human_play=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}