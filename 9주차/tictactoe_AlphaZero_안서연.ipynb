{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IDFKQRW-PKSk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from pathlib import Path\n",
        "from math import sqrt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# State"
      ],
      "metadata": {
        "id": "4G-sfe_RZ3FM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDITBkibLCzc",
        "outputId": "83431f04-0ffd-4948-dc80-0e9838c0aa74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o--\n",
            "---\n",
            "---\n",
            "\n",
            "\n",
            "o--\n",
            "-x-\n",
            "---\n",
            "\n",
            "\n",
            "o--\n",
            "-x-\n",
            "-o-\n",
            "\n",
            "\n",
            "o-x\n",
            "-x-\n",
            "-o-\n",
            "\n",
            "\n",
            "o-x\n",
            "-x-\n",
            "-oo\n",
            "\n",
            "\n",
            "o-x\n",
            "-x-\n",
            "xoo\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class TicTacToeState():\n",
        "    def __init__(self, pieces=None, enemy_pieces=None):\n",
        "        # 3x3 보드 초기화. None이면 빈 보드 생성.\n",
        "        self.pieces = pieces if pieces != None else [0] * 9\n",
        "        self.enemy_pieces = enemy_pieces if enemy_pieces != None else [0] * 9\n",
        "\n",
        "    # 돌의 수 얻기\n",
        "    def piece_count(self, pieces):\n",
        "        count = 0\n",
        "        for i in pieces:\n",
        "            if i == 1:\n",
        "                count += 1\n",
        "        return count\n",
        "\n",
        "    def check_line(self, pieces):\n",
        "        winning_lines = [\n",
        "            [0, 1, 2], # 가로\n",
        "            [3, 4, 5],\n",
        "            [6, 7, 8],\n",
        "            [0, 3, 6], # 세로\n",
        "            [1, 4, 7],\n",
        "            [2, 5, 8],\n",
        "            [0, 4, 8],# 대각선\n",
        "            [2, 4, 6]\n",
        "        ]\n",
        "        for line in winning_lines:\n",
        "            if all(pieces[i] == 1 for i in line):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # 패배 여부 판정 Q. 승리 여부를 판정하지 않고 패배 여부를 판정하는 이유?\n",
        "    def is_lose(self):\n",
        "        return self.check_line(self.enemy_pieces)\n",
        "\n",
        "    # 무승부 여부 확인\n",
        "    def is_draw(self):\n",
        "        total_pieces = [max(p, e) for p, e in zip(self.pieces, self.enemy_pieces)]\n",
        "        if all(p == 1 for p in total_pieces): # 모든 간이 채워졌는지 확인\n",
        "            # 승리나 패배 상태가 아닌 경우만 무승\n",
        "            if not self.check_line(self.pieces) and not self.check_line(self.enemy_pieces):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # 게임 종료 여부 확인\n",
        "    def is_done(self):\n",
        "        return self.is_lose() or self.is_draw()\n",
        "\n",
        "    # 다음 상태 얻기\n",
        "    def next(self, action):\n",
        "        pieces = self.pieces.copy()\n",
        "        pieces[action] = 1\n",
        "        return TicTacToeState(self.enemy_pieces, pieces)\n",
        "\n",
        "    # 가능한 수의 리스트 얻기\n",
        "    def legal_actions(self):\n",
        "        actions = []\n",
        "        for i in range(9):\n",
        "            if self.pieces[i] == 0 and self.enemy_pieces[i] == 0:\n",
        "                actions.append(i)\n",
        "        return actions\n",
        "\n",
        "    # 선수 여부 확인\n",
        "    def is_first_player(self):\n",
        "        return self.piece_count(self.pieces) == self.piece_count(self.enemy_pieces)\n",
        "\n",
        "    # 판 문자열 출력\n",
        "    def __str__(self):\n",
        "        ox = ('o', 'x') if self.is_first_player() else ('x', 'o')\n",
        "        str = ''\n",
        "        for i in range(9):\n",
        "            if self.pieces[i] == 1:\n",
        "                str += ox[0]\n",
        "            elif self.enemy_pieces[i] == 1:\n",
        "                str += ox[1]\n",
        "            else:\n",
        "                str += '-'\n",
        "            if i % 3 == 2:\n",
        "                str += '\\n'\n",
        "        return str\n",
        "\n",
        "# 랜덤으로 행동 선택\n",
        "def random_action(state):\n",
        "    legal_actions = state.legal_actions()\n",
        "    return legal_actions[random.randint(0, len(legal_actions) - 1)]\n",
        "\n",
        "# 알파베타법을 활용한 상태 가치 계산\n",
        "def alpha_beta(state, alpha, beta):\n",
        "    # 패배 시 상태 가치 -1\n",
        "    if state.is_lose():\n",
        "        return -1\n",
        "\n",
        "    # 무승부 시, 상태 가치 0\n",
        "    if state.is_draw():\n",
        "        return 0\n",
        "\n",
        "    # 가능한 수의 상태 가치 계산\n",
        "    for action in state.legal_actions():\n",
        "        score = -alpha_beta(state.next(action), -beta, -alpha)\n",
        "        if score > alpha:\n",
        "            alpha = score\n",
        "\n",
        "        if alpha >= beta:\n",
        "            return alpha\n",
        "\n",
        "    # 합법적인 수의 상태 가치의 최댓값을 반환\n",
        "    return alpha\n",
        "\n",
        "# 알파베타법을 활용한 행동 선택\n",
        "def alpha_beta_action(state):\n",
        "    # 가능한 수의 상태 가치 계산\n",
        "    best_action = 0\n",
        "    alpha = -float('inf')\n",
        "    for action in state.legal_actions():\n",
        "        score = -alpha_beta(state.next(action), -beta, -alpha)\n",
        "        if score > alpha:\n",
        "            best_action = action\n",
        "            alpha = score\n",
        "\n",
        "    return best_action\n",
        "\n",
        "# 플레이아웃\n",
        "def playout(state):\n",
        "   # 패배 시, 상태 가치 -1\n",
        "    if state.is_lose():\n",
        "        return -1\n",
        "\n",
        "    # 무승부 시, 상태 가치 0\n",
        "    if state.is_draw():\n",
        "        return 0\n",
        "\n",
        "    # 다음 상태의 상태 가치\n",
        "    return -playout(state.next(random_action(state)))\n",
        "\n",
        "# 최댓값의 인덱스 반환\n",
        "def argmax(collection):\n",
        "    return collection.index(max(collection))\n",
        "\n",
        "def mcts_action(state):\n",
        "    root_node = Node(state)\n",
        "    root_node.expand()\n",
        "\n",
        "    # 시뮬레이션 100회 반복 -> 시행횟수가 가장 큰 행동을 다음 수로 선택\n",
        "    for _ in range(100):\n",
        "        root_node.evaluate()\n",
        "\n",
        "    legal_actions = state.legal_actions()\n",
        "    n_list = []\n",
        "    for c in root_node.child_nodes:\n",
        "        n_list.append(c.n)\n",
        "    return legal_actions[argmax(n_list)]\n",
        "\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    # 상태 생성\n",
        "    state = TicTacToeState()\n",
        "\n",
        "    # 게임 종료 시까지 반복\n",
        "    while True:\n",
        "        # 게임 종료 시\n",
        "        if state.is_done():\n",
        "            break\n",
        "\n",
        "        # 다음 상태 얻기\n",
        "        state = state.next(random_action(state))\n",
        "\n",
        "        # 문자열 표시\n",
        "        print(state)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### State Test"
      ],
      "metadata": {
        "id": "QqnfWEAPbnHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 초기 상태\n",
        "state = TicTacToeState()\n",
        "assert state.pieces == [0] * 9\n",
        "assert state.enemy_pieces == [0] * 9\n",
        "assert state.is_done() == False\n",
        "assert state.legal_actions() == list(range(9))\n",
        "print(\"Initial state test completed.\")\n",
        "\n",
        "# 2. 승리 조건\n",
        "state = TicTacToeState(pieces=[1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
        "assert state.check_line(state.pieces) == True\n",
        "assert state.is_done() == False\n",
        "assert state.is_lose() == False\n",
        "print(\"Winning state test completed.\")\n",
        "\n",
        "# 3. 패배 조건\n",
        "state = TicTacToeState(enemy_pieces=[1, 0, 0, 0, 1, 0, 0, 0, 1])\n",
        "assert state.is_lose() == True\n",
        "assert state.is_done() == True\n",
        "print(\"Losing state test completed.\")\n",
        "\n",
        "# 4. 무승부 조건\n",
        "state = TicTacToeState(pieces=[1, 0, 1, 1, 1, 0, 0, 1, 0],\n",
        "                       enemy_pieces=[0, 1, 0, 0, 0, 1, 1, 0, 1]\n",
        "                       )\n",
        "assert state.is_draw() == True\n",
        "assert state.is_done() == True\n",
        "print(\"Draw state test completed.\")\n",
        "\n",
        "# 5. 다음 상태\n",
        "state = TicTacToeState()\n",
        "next_state = state.next(0)\n",
        "#print(next_state.pieces)\n",
        "#print(next_state.enemy_pieces)\n",
        "assert next_state.pieces == [0] * 9\n",
        "assert next_state.enemy_pieces == [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "print(\"Next state test completed.\")\n",
        "\n",
        "# 6. 가능한 행동 리스트\n",
        "state = TicTacToeState(\n",
        "    pieces=[1, 0, 0, 1, 1, 0, 0, 1, 1],\n",
        "    enemy_pieces=[0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
        ")\n",
        "#print(state)\n",
        "assert state.legal_actions() == [2, 5]\n",
        "print(\"Legal actions test passed.\")"
      ],
      "metadata": {
        "id": "P8tnYmWibros",
        "outputId": "672f0da0-3e27-4ae8-ea05-6f5fcc6d4c57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state test completed.\n",
            "Winning state test completed.\n",
            "Losing state test completed.\n",
            "Draw state test completed.\n",
            "Next state test completed.\n",
            "Legal actions test passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMcaVol7Dc1P"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "m7fq5V-3D2pY"
      },
      "outputs": [],
      "source": [
        "DN_FILTERS = 128 # convolutional layer 수\n",
        "DN_RESIDUAL_NUM = 16 # residual block 수\n",
        "DN_INPUT_SHAPE = (3, 3, 2) # 3x3의 2차원 배열 2개\n",
        "DN_OUTPUT_SIZE = 9 # 행동 수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KPT6PXRFIgCL"
      },
      "outputs": [],
      "source": [
        "# convolutional layer 정의\n",
        "def conv(filters):\n",
        "    return nn.Conv2d(\n",
        "        in_channels=filters,\n",
        "        out_channels=filters,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        bias=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "xUqd5HXVEOSG"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, filters, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        \"\"\"\n",
        "        - inplanes: input channel size\n",
        "        - planes: output channel size\n",
        "        - groups, base_width: ResNet이나 Wide ResNet의 경우 사용\n",
        "        \"\"\"\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "\n",
        "        # Basic Block의 구조\n",
        "        self.conv1 = conv(filters) # conv1에서 downsample\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv(filters)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # short connection (다운샘플링이 필요한 경우 수행)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "\n",
        "        # identity mapping 시 identity mapping 후 ReLU를 적용\n",
        "        # ReLU를 통과하면 양의 값만 남기 때문에 residual의 의미가 제대로 유지되지 않기 때문\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsHGFDp2Hzqz",
        "outputId": "bc4939ca-2194-4a30-8b9f-e53da9b21fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DualNetwork(\n",
            "  (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (residual_blocks): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (13): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (14): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (15): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (policy_head): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
            "    (2): Softmax(dim=1)\n",
            "  )\n",
            "  (value_head): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
            "    (2): Tanh()\n",
            "  )\n",
            ")\n",
            "Model loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-f21584f67f17>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path))\n"
          ]
        }
      ],
      "source": [
        "class DualNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualNetwork, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=DN_INPUT_SHAPE[0],\n",
        "            out_channels=DN_FILTERS,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(DN_FILTERS)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.residual_blocks = nn.Sequential(*[ResidualBlock(DN_FILTERS, DN_FILTERS, DN_FILTERS) for _ in range(DN_RESIDUAL_NUM)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(DN_FILTERS, DN_OUTPUT_SIZE),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(DN_FILTERS, 1),\n",
        "            nn.Tanh()\n",
        "        ) # Sequential?\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.residual_blocks(x)\n",
        "\n",
        "        x = self.global_pool(x)\n",
        "\n",
        "        p = self.policy_head(x)\n",
        "        v = self.value_head(x)\n",
        "\n",
        "        return p, v\n",
        "\n",
        "def save_model(model, path='./model/best.pth'):\n",
        "    os.makedirs('./model/', exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_model(path='./model/best.pth'):\n",
        "    model = DualNetwork()\n",
        "    if os.path.exists(path):\n",
        "        model.load_state_dict(torch.load(path))\n",
        "    model.eval() # 추론 모드로 설정\n",
        "    return model\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    model = DualNetwork()\n",
        "    print(model)\n",
        "\n",
        "    # 모델 저장 테스트\n",
        "    save_model(model)\n",
        "\n",
        "    # 모델 로드 테스트\n",
        "    loaded_model = load_model()\n",
        "    print(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network Test"
      ],
      "metadata": {
        "id": "LM5wLdo6h96Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = torch.randn(1, DN_INPUT_SHAPE[0], DN_INPUT_SHAPE[1], DN_INPUT_SHAPE[2])\n",
        "if __name__ == '__main__':\n",
        "    model = DualNetwork()\n",
        "    p, v = model(test_input)\n",
        "    print(p.shape)\n",
        "    print(v.shape)\n",
        "    print(p)\n",
        "    print(v)"
      ],
      "metadata": {
        "id": "Zz0AKXzYiF3I",
        "outputId": "08454312-61df-4916-bb9c-0f7815343457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 9])\n",
            "torch.Size([1, 1])\n",
            "tensor([[0.0109, 0.0716, 0.4173, 0.1787, 0.0223, 0.1263, 0.0317, 0.1056, 0.0356]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[-0.5485]], grad_fn=<TanhBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCTS Node"
      ],
      "metadata": {
        "id": "qFX5fwlJT_17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, state, p):\n",
        "        self.state = state\n",
        "        self.p = p\n",
        "        self.w = 0\n",
        "        self.n = 0\n",
        "        self.child_nodes = None\n",
        "\n",
        "    # 평가\n",
        "    def evaluate(self):\n",
        "        # 게임 종료 시\n",
        "        if self.state.is_done():\n",
        "            value = -1 if self.state.is_lose() else 0\n",
        "\n",
        "            # 갱신\n",
        "            self.w += value\n",
        "            self.n += 1\n",
        "            return value\n",
        "\n",
        "        # 리프노드에 도달 시\n",
        "        if not self.child_nodes:\n",
        "            # neural network 추론을 통해 정책과 가치 얻기\n",
        "            policies, value = predict(model, self.state)\n",
        "\n",
        "            # 갱신\n",
        "            self.w += value\n",
        "            self.n += 1\n",
        "\n",
        "            # 전개\n",
        "            self.child_nodes = []\n",
        "            for action, policy in zip(self.state. legal_actions(), policies):\n",
        "                self.child_nodes.append(Node(self.state.next(action), policy))\n",
        "            return value\n",
        "\n",
        "        else: # 자녀 노드 있는 경우\n",
        "            # 아크 평가값이 가장 큰 자녀 노드를 평가해 가치 얻기\n",
        "            value = -self.next_child_node().evaluate()\n",
        "\n",
        "            self.w += value\n",
        "            self.n += 1\n",
        "            return value\n",
        "\n",
        "    # 아크 평가값이 가장 큰 자녀 노드 얻기\n",
        "    def next_child_node(self):\n",
        "        # 시행 횟수 n이 0인 자녀 노드 반환\n",
        "        for child_node in self.child_nodes:\n",
        "            if child_node.n == 0:\n",
        "                return child_node\n",
        "\n",
        "        # 아크 평가 계산\n",
        "        C_PUCT = 1.0\n",
        "        t = sum(nodes_to_scores(self.child_nodes))\n",
        "        pucb_values = []\n",
        "        for child_node in self.child_nodes:\n",
        "            pucb_values.append((-child_node.w / child_node.n if child_node.n else 0.0) + C_PUCT * child_node.p * sqrt(t) / (1 + child_node.n))\n",
        "\n",
        "        # 아크 평가값이 가장 큰 자녀 노드 반환\n",
        "        return self.child_nodes[argmax(pucb_values)]"
      ],
      "metadata": {
        "id": "31X9kHAhUC6_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Node Test"
      ],
      "metadata": {
        "id": "GjGkkjMpjeVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = TicTacToeState()\n",
        "state.done = False\n",
        "state.lose = False\n",
        "state.legal_actions_list = state.legal_actions if state.legal_actions is not None else [0, 1, 2]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    root_node = Node(state, p=1.0)\n",
        "\n",
        "    value = root_node.evaluate()\n",
        "    print(value)\n",
        "    print(root_node.w)\n",
        "    print(root_node.n)\n",
        "\n",
        "    if root_node.child_nodes:\n",
        "        print(\"자녀 노드 있음\")\n",
        "        print(len(root_node.child_nodes))\n",
        "\n",
        "    best_child = root_node.next_child_node()\n",
        "    print(best_child.p)\n",
        "    print(best_child.w)\n",
        "    print(best_child.n)"
      ],
      "metadata": {
        "id": "yKCkh6eajgB_",
        "outputId": "156f8163-8cbb-43ab-a591-e73eaa1c1a47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.07290931791067123\n",
            "-0.07290931791067123\n",
            "1\n",
            "자녀 노드 있음\n",
            "9\n",
            "0.11248944\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCTS"
      ],
      "metadata": {
        "id": "fUBEBT7I_ajK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PV_EVALUATE_COUNT = 50 # predict 1회당 시뮬레이션 횟수"
      ],
      "metadata": {
        "id": "E9RDbS1O_dCe"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 추론\n",
        "def predict(model, state):\n",
        "    # 추론을 위한 입력 데이터 shape 변환\n",
        "    a, b, c = DN_INPUT_SHAPE\n",
        "    x = np.array([state.pieces, state.enemy_pieces]) # (2, H, W)\n",
        "    x = x.reshape(c, a, b).transpose(1, 2, 0).reshape(1, a, b, c) # pytorch: 채널 우선 형식 (N, C, H, W)\n",
        "    # reshape은 데이터 순서를 바꾸지 않고, 단순히 numpy 배열의 재구성만 수행하여 차원을 재조정함\n",
        "    # transpose은 실제 데이터 순서를 변경\n",
        "    x_tensor = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "    # 추론\n",
        "    with torch.no_grad():\n",
        "        y_policy, y_value = model(x_tensor)\n",
        "\n",
        "    # 정책\n",
        "    policies = y_policy[0][list(state.legal_actions())].numpy() # 가능한 수만 추출\n",
        "    policies /= sum(policies) if sum(policies) else 1 # 확률 분포로 변환\n",
        "\n",
        "    # 가치\n",
        "    value = y_value[0].item()\n",
        "    return policies, value\n",
        "\n",
        "def nodes_to_scores(nodes):\n",
        "    scores = []\n",
        "    for c in nodes:\n",
        "        scores.append(c.n)\n",
        "    return scores\n",
        "\n",
        "def get_policy(model, state, temperature): # scores -> policy\n",
        "    # 현재 상태의 노드 생성\n",
        "    root_node = Node(state, 0)\n",
        "\n",
        "    for _ in range(PV_EVALUATE_COUNT):\n",
        "        root_node.evaluate()\n",
        "\n",
        "    scores = nodes_to_scores(root_node.child_nodes)\n",
        "    # temperature == 0 : 가장 높은 확률을 가진 행동을 선택\n",
        "    # temperature > 0 : 확률 분포에 따라 무작위 선택, 값이 클수록 무작위성이 커짐\n",
        "    if temperature == 0:\n",
        "        action = np.argmax(scores)\n",
        "        scores = np.zeros(len(scores))\n",
        "        scores[action] = 1\n",
        "    else: # 볼츠만 함수로 variation 추가\n",
        "        scores = boltzman(scores, temperature)\n",
        "    return scores\n",
        "\n",
        "def pv_mcts_action(model, temperature=0):\n",
        "    def pv_mcts_action(state):\n",
        "        scores = get_policy(model, state, temperature)\n",
        "        return np.random.choice(state.legal_actions(), p=scores) # p=scores: 행동 선택이 점수확률에 비례하도\n",
        "\n",
        "    return pv_mcts_action\n",
        "\n",
        "# 볼츠반 분포\n",
        "def boltzman(xs, temperature):\n",
        "    xs = [x ** (1 / temperature) for x in xs]\n",
        "    return [x / sum(xs) for x in xs]\n"
      ],
      "metadata": {
        "id": "W1gQn_YK_nmB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MCTS Test"
      ],
      "metadata": {
        "id": "UJpSU6ijlHdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # 모델 로드\n",
        "    model_path = './model/best.pth'\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model file not found at {model_path}. Please save the model first.\")\n",
        "\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # 상태 생성\n",
        "    state = TicTacToeState()\n",
        "\n",
        "    next_action = pv_mcts_action(model, 1.0)\n",
        "\n",
        "    # 게임 종료 시까지 반복\n",
        "    while True:\n",
        "        # 게임 종료 시\n",
        "        if state.is_done():\n",
        "            break\n",
        "\n",
        "        # 행동 얻기\n",
        "        action = next_action(state)\n",
        "\n",
        "        # 다음 상태 얻기\n",
        "        state = state.next(action)\n",
        "\n",
        "        # 문장열 출력\n",
        "        print(state)"
      ],
      "metadata": {
        "id": "z73ltWkzlElp",
        "outputId": "074daced-d238-40bf-ea76-a39415004304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-f21584f67f17>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "o--\n",
            "---\n",
            "\n",
            "---\n",
            "o--\n",
            "x--\n",
            "\n",
            "--o\n",
            "o--\n",
            "x--\n",
            "\n",
            "--o\n",
            "o--\n",
            "xx-\n",
            "\n",
            "o-o\n",
            "o--\n",
            "xx-\n",
            "\n",
            "o-o\n",
            "o--\n",
            "xxx\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-play"
      ],
      "metadata": {
        "id": "2E6aiawS1qne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SP_GAME_COUNT = 50 # self-play 수행할 게임 수\n",
        "SP_TEMPERATURE = 1.0"
      ],
      "metadata": {
        "id": "tZX-1ji6BLIj"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_player_value(ended_state):\n",
        "    # 1: 선 수 플레이어 승리, -1: 선 수 플레이어 패배, 0: 무승부\n",
        "    if ended_state.is_lose():\n",
        "        return -1 if ended_state.is_first_player() else 1\n",
        "    return 0\n",
        "\n",
        "# self-play를 통해 수집한 학습 데이터 저장\n",
        "def save_data(data, path='./data/train_data.npy'):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    if os.path.exists(path):\n",
        "        existing_data = np.load(path, allow_pickle=True)\n",
        "        data = np.concatenate([existing_data, data], axis=0)\n",
        "    np.save(path, data)\n",
        "    print(f\"Data saved to {path}, total size: {len(data)}\")\n",
        "\n",
        "def self_play():\n",
        "    # 학습 데이터\n",
        "    train_data = []\n",
        "\n",
        "    # 베스트 플레이어 모델로 self-play\n",
        "    model = load_model('./model/best.pth')\n",
        "\n",
        "    for i in range(SP_GAME_COUNT):\n",
        "        state = TicTacToeState()\n",
        "\n",
        "        # 각 상태마다 [state.pieces, state.enemy_pieces], polices, result 저장\n",
        "        game_data = []\n",
        "\n",
        "        # 게임 진행\n",
        "        while True:\n",
        "            if state.is_done():\n",
        "                break\n",
        "\n",
        "            scores = get_policy(model, state, SP_TEMPERATURE)\n",
        "\n",
        "            policies = [0] * DN_OUTPUT_SIZE\n",
        "            for action, policy in zip(state.legal_actions(), scores):\n",
        "                policies[action] = policy\n",
        "            game_data.append([[state.pieces, state.enemy_pieces], policies, None]) # result는 아직 할당 안됨\n",
        "\n",
        "            action = np.random.choice(state.legal_actions(), p=scores)\n",
        "            state = state.next(action)\n",
        "\n",
        "        value = first_player_value(state)\n",
        "\n",
        "        # 역방향으로 각 상태에 게임 result 저장\n",
        "        for j in range(len(game_data)):\n",
        "            game_data[j][2] = value\n",
        "            value = -value\n",
        "\n",
        "        # 현재 게임 데이터를 train_data에 추가\n",
        "        train_data.extend(game_data)\n",
        "\n",
        "        # 출력\n",
        "        print(f\"Game {i + 1}/{SP_GAME_COUNT} completed.\")\n",
        "\n",
        "        #del model\n",
        "        #torch.cuda.empty_cache()\n",
        "\n",
        "    # 학습 데이터 저장\n",
        "    save_data(np.array(train_data, dtype=object))\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    self_play()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR0Ywz0I1qB6",
        "outputId": "1f9505ff-ad83-4619-f83f-33ea34a07896"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-f21584f67f17>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Game 1/50 completed.\n",
            "Game 2/50 completed.\n",
            "Game 3/50 completed.\n",
            "Game 4/50 completed.\n",
            "Game 5/50 completed.\n",
            "Game 6/50 completed.\n",
            "Game 7/50 completed.\n",
            "Game 8/50 completed.\n",
            "Game 9/50 completed.\n",
            "Game 10/50 completed.\n",
            "Game 11/50 completed.\n",
            "Game 12/50 completed.\n",
            "Game 13/50 completed.\n",
            "Game 14/50 completed.\n",
            "Game 15/50 completed.\n",
            "Game 16/50 completed.\n",
            "Game 17/50 completed.\n",
            "Game 18/50 completed.\n",
            "Game 19/50 completed.\n",
            "Game 20/50 completed.\n",
            "Game 21/50 completed.\n",
            "Game 22/50 completed.\n",
            "Game 23/50 completed.\n",
            "Game 24/50 completed.\n",
            "Game 25/50 completed.\n",
            "Game 26/50 completed.\n",
            "Game 27/50 completed.\n",
            "Game 28/50 completed.\n",
            "Game 29/50 completed.\n",
            "Game 30/50 completed.\n",
            "Game 31/50 completed.\n",
            "Game 32/50 completed.\n",
            "Game 33/50 completed.\n",
            "Game 34/50 completed.\n",
            "Game 35/50 completed.\n",
            "Game 36/50 completed.\n",
            "Game 37/50 completed.\n",
            "Game 38/50 completed.\n",
            "Game 39/50 completed.\n",
            "Game 40/50 completed.\n",
            "Game 41/50 completed.\n",
            "Game 42/50 completed.\n",
            "Game 43/50 completed.\n",
            "Game 44/50 completed.\n",
            "Game 45/50 completed.\n",
            "Game 46/50 completed.\n",
            "Game 47/50 completed.\n",
            "Game 48/50 completed.\n",
            "Game 49/50 completed.\n",
            "Game 50/50 completed.\n",
            "Data saved to ./data/train_data.npy, total size: 704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "0p08zEZgCCof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RN_EPOCHS = 100 # 학습 횟수"
      ],
      "metadata": {
        "id": "Y1lCPE184JEj"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 및 옵티마이저 정의\n",
        "criterion_for_policy = nn.CrossEntropyLoss()\n",
        "criterion_for_value = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "BN5fjqbOF0yB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 준비\n",
        "def load_data(path='./data/train_data.npy'):\n",
        "    # save_data에서 저장된 경로를 바로 사용하도록 수정...\n",
        "    try:\n",
        "        data = np.load(path, allow_pickle=True)\n",
        "        print(f\"Data loaded from {path}, total size: {len(data)}\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found at {path}. Make sure the data exists.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_data(data):\n",
        "    xs, y_policies, y_values = zip(*data)\n",
        "\n",
        "    a, b, c = DN_INPUT_SHAPE\n",
        "    xs = np.array(xs)\n",
        "    xs = xs.reshape(len(xs), c, a, b).transpose(0, 2, 3, 1) # pytorch: 채널 우선 형식 (N, C, H, W)\n",
        "    y_policies = np.array(y_policies)\n",
        "    y_values = np.array(y_values)\n",
        "\n",
        "    # 데이터를 텐서로 변환\n",
        "    xs = torch.tensor(xs, dtype=torch.float32)\n",
        "    y_policies = torch.tensor(y_policies, dtype=torch.float32)\n",
        "    y_values = torch.tensor(y_values, dtype=torch.float32)\n",
        "\n",
        "    # 데이터셋과 데이터로더 준비\n",
        "    dataset = TensorDataset(xs, y_policies, y_values)\n",
        "    train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    return train_loader"
      ],
      "metadata": {
        "id": "qvZXfFzbCNI4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network():\n",
        "    train_data = load_data()\n",
        "    train_loader = prepare_data(train_data)\n",
        "\n",
        "    model = load_model('./model/best.pth')\n",
        "\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #model.to(device)\n",
        "\n",
        "    # 학습 스케쥴러 설정\n",
        "    def step_decay(epoch):\n",
        "        if epoch >= 80:\n",
        "            return 0.0005\n",
        "        elif epoch >= 100:\n",
        "            return 0.00025\n",
        "        return 0.001\n",
        "\n",
        "    epoch_losses = []\n",
        "\n",
        "    # 학습 실행\n",
        "    for epoch in range(RN_EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, policies, values in train_loader:\n",
        "            #inputs, policies, values = inputs.to(device), policies.to(device), values.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            policy_pred, value_pred = model(inputs)\n",
        "\n",
        "            loss_policy = criterion_for_policy(policy_pred, policies)\n",
        "            loss_value = criterion_for_value(value_pred, values.view(-1, 1)) # 타겟 크기 수\n",
        "            policy_loss_weight = 1.0\n",
        "            value_loss_weight = 0.01\n",
        "            total_loss = policy_loss_weight * loss_policy + value_loss_weight * loss_value\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 손실 누적\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "        # 학습 스케쥴링\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = step_decay(epoch)\n",
        "\n",
        "        epoch_losses.append(running_loss / len(train_loader))\n",
        "        print(f'Epoch [{epoch + 1}/{RN_EPOCHS}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "\n",
        "    # 최신 플레이어 모델 저장\n",
        "    torch.save(model.state_dict(), '/model/latest.pth')\n",
        "\n",
        "    # 손실 그래프 시각화\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, RN_EPOCHS + 1), epoch_losses, label=\"Training Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # del model\n",
        "    # torch.cuda.empty_cache()\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    train_network()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tlF4RcoZHqqi",
        "outputId": "62d3fceb-915a-49f3-ad4e-89794a1217ac"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded from ./data/train_data.npy, total size: 704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-f21584f67f17>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.2222\n",
            "Epoch [2/100], Loss: 2.2226\n",
            "Epoch [3/100], Loss: 2.2235\n",
            "Epoch [4/100], Loss: 2.2245\n",
            "Epoch [5/100], Loss: 2.2230\n",
            "Epoch [6/100], Loss: 2.2224\n",
            "Epoch [7/100], Loss: 2.2232\n",
            "Epoch [8/100], Loss: 2.2237\n",
            "Epoch [9/100], Loss: 2.2233\n",
            "Epoch [10/100], Loss: 2.2234\n",
            "Epoch [11/100], Loss: 2.2230\n",
            "Epoch [12/100], Loss: 2.2229\n",
            "Epoch [13/100], Loss: 2.2226\n",
            "Epoch [14/100], Loss: 2.2227\n",
            "Epoch [15/100], Loss: 2.2236\n",
            "Epoch [16/100], Loss: 2.2223\n",
            "Epoch [17/100], Loss: 2.2230\n",
            "Epoch [18/100], Loss: 2.2227\n",
            "Epoch [19/100], Loss: 2.2227\n",
            "Epoch [20/100], Loss: 2.2236\n",
            "Epoch [21/100], Loss: 2.2226\n",
            "Epoch [22/100], Loss: 2.2232\n",
            "Epoch [23/100], Loss: 2.2237\n",
            "Epoch [24/100], Loss: 2.2237\n",
            "Epoch [25/100], Loss: 2.2233\n",
            "Epoch [26/100], Loss: 2.2233\n",
            "Epoch [27/100], Loss: 2.2228\n",
            "Epoch [28/100], Loss: 2.2219\n",
            "Epoch [29/100], Loss: 2.2232\n",
            "Epoch [30/100], Loss: 2.2222\n",
            "Epoch [31/100], Loss: 2.2231\n",
            "Epoch [32/100], Loss: 2.2234\n",
            "Epoch [33/100], Loss: 2.2224\n",
            "Epoch [34/100], Loss: 2.2231\n",
            "Epoch [35/100], Loss: 2.2220\n",
            "Epoch [36/100], Loss: 2.2239\n",
            "Epoch [37/100], Loss: 2.2249\n",
            "Epoch [38/100], Loss: 2.2220\n",
            "Epoch [39/100], Loss: 2.2225\n",
            "Epoch [40/100], Loss: 2.2223\n",
            "Epoch [41/100], Loss: 2.2232\n",
            "Epoch [42/100], Loss: 2.2230\n",
            "Epoch [43/100], Loss: 2.2229\n",
            "Epoch [44/100], Loss: 2.2226\n",
            "Epoch [45/100], Loss: 2.2222\n",
            "Epoch [46/100], Loss: 2.2233\n",
            "Epoch [47/100], Loss: 2.2219\n",
            "Epoch [48/100], Loss: 2.2243\n",
            "Epoch [49/100], Loss: 2.2233\n",
            "Epoch [50/100], Loss: 2.2231\n",
            "Epoch [51/100], Loss: 2.2224\n",
            "Epoch [52/100], Loss: 2.2214\n",
            "Epoch [53/100], Loss: 2.2224\n",
            "Epoch [54/100], Loss: 2.2232\n",
            "Epoch [55/100], Loss: 2.2229\n",
            "Epoch [56/100], Loss: 2.2238\n",
            "Epoch [57/100], Loss: 2.2230\n",
            "Epoch [58/100], Loss: 2.2240\n",
            "Epoch [59/100], Loss: 2.2239\n",
            "Epoch [60/100], Loss: 2.2239\n",
            "Epoch [61/100], Loss: 2.2228\n",
            "Epoch [62/100], Loss: 2.2232\n",
            "Epoch [63/100], Loss: 2.2241\n",
            "Epoch [64/100], Loss: 2.2241\n",
            "Epoch [65/100], Loss: 2.2227\n",
            "Epoch [66/100], Loss: 2.2226\n",
            "Epoch [67/100], Loss: 2.2221\n",
            "Epoch [68/100], Loss: 2.2248\n",
            "Epoch [69/100], Loss: 2.2228\n",
            "Epoch [70/100], Loss: 2.2224\n",
            "Epoch [71/100], Loss: 2.2226\n",
            "Epoch [72/100], Loss: 2.2231\n",
            "Epoch [73/100], Loss: 2.2233\n",
            "Epoch [74/100], Loss: 2.2218\n",
            "Epoch [75/100], Loss: 2.2228\n",
            "Epoch [76/100], Loss: 2.2224\n",
            "Epoch [77/100], Loss: 2.2240\n",
            "Epoch [78/100], Loss: 2.2221\n",
            "Epoch [79/100], Loss: 2.2240\n",
            "Epoch [80/100], Loss: 2.2228\n",
            "Epoch [81/100], Loss: 2.2234\n",
            "Epoch [82/100], Loss: 2.2229\n",
            "Epoch [83/100], Loss: 2.2235\n",
            "Epoch [84/100], Loss: 2.2233\n",
            "Epoch [85/100], Loss: 2.2240\n",
            "Epoch [86/100], Loss: 2.2227\n",
            "Epoch [87/100], Loss: 2.2233\n",
            "Epoch [88/100], Loss: 2.2241\n",
            "Epoch [89/100], Loss: 2.2238\n",
            "Epoch [90/100], Loss: 2.2236\n",
            "Epoch [91/100], Loss: 2.2229\n",
            "Epoch [92/100], Loss: 2.2217\n",
            "Epoch [93/100], Loss: 2.2230\n",
            "Epoch [94/100], Loss: 2.2230\n",
            "Epoch [95/100], Loss: 2.2226\n",
            "Epoch [96/100], Loss: 2.2228\n",
            "Epoch [97/100], Loss: 2.2230\n",
            "Epoch [98/100], Loss: 2.2227\n",
            "Epoch [99/100], Loss: 2.2220\n",
            "Epoch [100/100], Loss: 2.2229\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory /model does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-91c1374162c6>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# 동작 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-91c1374162c6>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# 최신 플레이어 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/model/latest.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# 손실 그래프 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /model does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Player"
      ],
      "metadata": {
        "id": "a7vLKMRXmdyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EP_GAME_COUNT = 10  # 평가 1회당 게임 수"
      ],
      "metadata": {
        "id": "oxpji1u6mcu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 선 수를 둔 플레이어의 포인트\n",
        "def first_player_point(ended_state):\n",
        "    # 1: 선 수 플레이어 승리, 0: 선 수 플레이어 패배, 0.5: 무승부\n",
        "    if ended_state.is_lose():\n",
        "        return 0 if ended_state.is_first_player() else 1\n",
        "    return 0.5\n",
        "\n",
        "\n",
        "# 1 게임 실행\n",
        "def play(next_actions):\n",
        "    # 상태 생성\n",
        "    state = TicTacToeState()\n",
        "\n",
        "    # 게임 종료 시까지 반복\n",
        "    while True:\n",
        "        # 게임 종료 시\n",
        "        if state.is_done():\n",
        "            break\n",
        "\n",
        "        # 행동 얻기\n",
        "        next_action = next_actions[0] if state.is_first_player() else next_actions[1]\n",
        "        action = next_action(state)\n",
        "\n",
        "        # 다음 상태의 획득\n",
        "        state = state.next(action)\n",
        "\n",
        "    # 선 수 플레이어의 포인트 반환\n",
        "    return first_player_point(state)\n",
        "\n",
        "\n",
        "# 임의의 알고리즘 평가\n",
        "def evaluate_algorithm_of(label, next_actions):\n",
        "    # 여러 차례 대전을 반복\n",
        "    total_point = 0\n",
        "    for i in range(EP_GAME_COUNT):\n",
        "        # 1 게임 실행\n",
        "        if i % 2 == 0:\n",
        "            total_point += play(next_actions)\n",
        "        else:\n",
        "            total_point += 1 - play(list(reversed(next_actions)))\n",
        "\n",
        "        # 출력\n",
        "        print('\\rEvaluate {}/{}'.format(i + 1, EP_GAME_COUNT), end='')\n",
        "    print('')\n",
        "\n",
        "    # 평균 포인트 계산\n",
        "    average_point = total_point / EP_GAME_COUNT\n",
        "    print(label, average_point)\n",
        "\n",
        "\n",
        "# 베스트 플레이어 평가\n",
        "def evaluate_best_player():\n",
        "    # 베스트 플레이어 모델 로드 (PyTorch로 변경)\n",
        "    model = torch.load('./model/best.pth')\n",
        "    model.eval()  # 평가 모드로 설정\n",
        "\n",
        "    # PV MCTS로 행동 선택을 수행하는 함수 생성\n",
        "    next_pv_mcts_action = pv_mcts_action(model, 0.0)\n",
        "\n",
        "    # VS 랜덤\n",
        "    next_actions = (next_pv_mcts_action, random_action)\n",
        "    evaluate_algorithm_of('VS_Random', next_actions)\n",
        "\n",
        "    # VS 알파베타법\n",
        "    next_actions = (next_pv_mcts_action, alpha_beta_action)\n",
        "    evaluate_algorithm_of('VS_AlphaBeta', next_actions)\n",
        "\n",
        "    # VS 몬테카를로 트리 탐색\n",
        "    next_actions = (next_pv_mcts_action, mcts_action)\n",
        "    evaluate_algorithm_of('VS_MCTS', next_actions)\n",
        "\n",
        "\n",
        "# 동작 확인\n",
        "if __name__ == '__main__':\n",
        "    evaluate_best_player()"
      ],
      "metadata": {
        "id": "8X_82_q3mi4Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}