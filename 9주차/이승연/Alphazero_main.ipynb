{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 00. Import"
      ],
      "metadata": {
        "id": "Dm-Xx_M7yFLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J7gXEUNMyBaB"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "from math import sqrt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01. Environment"
      ],
      "metadata": {
        "id": "egilSLkVyHNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter\n",
        "state_size = (3,3)"
      ],
      "metadata": {
        "id": "L1Em0YLyyKXl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self, state_size:Tuple):\n",
        "        # env size\n",
        "        self.state_size = state_size # (3, 3)\n",
        "        self.n = self.state_size[0] # 3\n",
        "        self.num_actions = self.n ** 2 # 9\n",
        "\n",
        "        # state, action\n",
        "        self.present_state = np.zeros((2, self.n, self.n)) # present_state[0]: state for first player\n",
        "        self.action_space = np.arange(self.num_actions) # [0, 1, ..., 8] : action idx\n",
        "\n",
        "        # reward, done\n",
        "        self.reward_dict = {'win':1, 'lose':-1, 'draw':0, 'progress':0}\n",
        "        self.done = False\n",
        "\n",
        "        # 추가\n",
        "        self.player = True # True: first player\n",
        "\n",
        "\n",
        "    def step(self, action_idx):\n",
        "        '''\n",
        "        action_idx에 따라 게임 진행\n",
        "        output: next_state, reward, done, is_win\n",
        "        '''\n",
        "        x, y = divmod(action_idx, self.n)\n",
        "\n",
        "        self.change_player() # change turn\n",
        "        self.present_state[1][x, y] = -1\n",
        "\n",
        "        # 게임 종료 및 승자 확인\n",
        "        next_state = self.present_state\n",
        "        done, is_win = self.is_done(next_state)\n",
        "        reward = self.check_reward(is_win)\n",
        "        self.done = done\n",
        "\n",
        "        return next_state ,reward, done, is_win\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        game reset\n",
        "        '''\n",
        "        self.present_state = np.zeros((2, self.n, self.n))\n",
        "        self.done = False\n",
        "        self.player = True\n",
        "\n",
        "    def render(self, state):\n",
        "        '''\n",
        "        print by string\n",
        "        first player: X / second player: O\n",
        "        '''\n",
        "        state = state if self.player else state[[1, 0]]\n",
        "        state = state.reshape(2, -1)\n",
        "        board = state[0] - state[1] # -1: player / 1: enemy\n",
        "        check_board = list(map(lambda x: 'X' if board[x] == -1 else 'O' if board[x] == 1 else '.', self.action_space))\n",
        "\n",
        "        # string으로 변환하고 game board 형태로 출력\n",
        "        board_string = ' '.join(check_board)\n",
        "        formatted_string = '\\n'.join([board_string[i:i+6] for i in range(0, len(board_string), 6)])\n",
        "\n",
        "        print(formatted_string)\n",
        "        print(\"-\"*10)\n",
        "\n",
        "\n",
        "    def check_legal_action(self, state):\n",
        "        '''\n",
        "        board에서 가능한 action array를 원핫으로 출력\n",
        "        '''\n",
        "        state = state.reshape(2,-1)\n",
        "        board = state[0]+state[1]\n",
        "        legal_actions = np.array([board[x] == 0 for x in self.action_space], dtype = int)\n",
        "        return legal_actions\n",
        "\n",
        "\n",
        "    def is_done(self, state):\n",
        "        '''\n",
        "        game의 종료 여부 확인\n",
        "        is_win: True - win / False - draw\n",
        "        '''\n",
        "        is_done, is_win = False, False\n",
        "        player_state = state[1]\n",
        "\n",
        "        # 무승부 여부 확인\n",
        "        if np.sum(state) == -9:\n",
        "            is_done, is_win = True, False\n",
        "\n",
        "        # 승리 조건 확인\n",
        "        axis_diag_sum = np.concatenate([player_state.sum(axis=0), player_state.sum(axis=1), [player_state.trace()], [np.fliplr(player_state).trace()]]) # (8, )\n",
        "        if -3 in axis_diag_sum:\n",
        "            is_done, is_win = True, True\n",
        "\n",
        "        return is_done, is_win\n",
        "\n",
        "\n",
        "    # 추가 메서드\n",
        "    def change_player(self):\n",
        "        '''\n",
        "        state를 다음 턴으로 돌려줌\n",
        "        player를 다음 player로 바꿈\n",
        "        '''\n",
        "        self.present_state[[0, 1]] = self.present_state[[1, 0]]\n",
        "        self.player = not self.player\n",
        "\n",
        "\n",
        "    def check_reward(self, is_win):\n",
        "        '''\n",
        "        reward를 주는 함수\n",
        "        draw, progress: 0\n",
        "        first player 기준 reward 제공\n",
        "        player를 돌린 후 reward를 제공하는 것 고려\n",
        "        '''\n",
        "        reward = 0\n",
        "\n",
        "        if is_win:\n",
        "            reward = self.reward_dict[\"lose\"] if self.player else self.reward_dict[\"win\"]\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def choose_random_action(self, state):\n",
        "        '''\n",
        "        가능한 action 중에서 random으로 action을 선택한다.\n",
        "        '''\n",
        "        legal_actions = self.check_legal_action(state)\n",
        "        legal_action_idxs = np.where(legal_actions != 0)[0]\n",
        "        action = np.random.choice(legal_action_idxs)\n",
        "\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "qms7sjvtyuFu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "EUyOvV2TyzpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = (3,3)\n",
        "env = Environment(state_size)\n",
        "env.render(env.present_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdxAjtyHyyTf",
        "outputId": "da178698-96ed-44f4-8559-182acfb5b2cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". . . \n",
            ". . . \n",
            ". . .\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(env.present_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "porWwkdswSxY",
        "outputId": "572860f2-edba-4115-92c3-e4946ec60244"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "while not env.done:\n",
        "    action = env.choose_random_action(env.present_state)\n",
        "    next_state, reward, done, is_win = env.step(action)\n",
        "    print(reward, done, is_win, env.player)\n",
        "    env.render(next_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXjme1zuy7y0",
        "outputId": "bb272920-26fb-4d0e-f83f-eda578f2369c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 False False False\n",
            "X . . \n",
            ". . . \n",
            ". . .\n",
            "----------\n",
            "0 False False True\n",
            "X . . \n",
            ". . . \n",
            ". O .\n",
            "----------\n",
            "0 False False False\n",
            "X . . \n",
            ". . . \n",
            "X O .\n",
            "----------\n",
            "0 False False True\n",
            "X . . \n",
            "O . . \n",
            "X O .\n",
            "----------\n",
            "0 False False False\n",
            "X . . \n",
            "O . X \n",
            "X O .\n",
            "----------\n",
            "0 False False True\n",
            "X O . \n",
            "O . X \n",
            "X O .\n",
            "----------\n",
            "0 False False False\n",
            "X O X \n",
            "O . X \n",
            "X O .\n",
            "----------\n",
            "0 False False True\n",
            "X O X \n",
            "O . X \n",
            "X O O\n",
            "----------\n",
            "1 True True False\n",
            "X O X \n",
            "O X X \n",
            "X O O\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02. Net"
      ],
      "metadata": {
        "id": "8ta5wqJiyLnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter\n",
        "state_size = (3, 3) # env.state_size\n",
        "action_size = 9 # env.action_size\n",
        "\n",
        "CONV_UNITS = 64\n",
        "RESIDUAL_NUM = 16\n",
        "BATCHSIZE = 64"
      ],
      "metadata": {
        "id": "u5PJBCh0zZgF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,3), bias=False, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sc = x\n",
        "\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x += sc\n",
        "        x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8--CpKpEzmsI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main Net\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, state_size, action_size, conv_units):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=2, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(conv_units)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(3,3), stride=1, padding=1)\n",
        "        self.residual_block = ResidualBlock(conv_units, conv_units)\n",
        "\n",
        "        self.batch_size = BATCHSIZE\n",
        "\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Conv2d(conv_units, 2, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(18, action_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Conv2d(conv_units, 1, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(9, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # residual block\n",
        "        for i in range(RESIDUAL_NUM):\n",
        "            x = self.residual_block(x)\n",
        "\n",
        "        # pooling\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # policy, value 반환\n",
        "        policy = self.policy_head(x)\n",
        "        value = self.value_head(x)\n",
        "\n",
        "        return policy, value"
      ],
      "metadata": {
        "id": "kmfy9JQGzcZj"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "rAVZGTjx2guO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = (3,3)\n",
        "env = Environment(state_size)\n",
        "model = Net(state_size, env.num_actions, CONV_UNITS)"
      ],
      "metadata": {
        "id": "YlH8ixjA2je3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jb2olSC3OlK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03. MCTS"
      ],
      "metadata": {
        "id": "8zelf2uVyOJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter\n",
        "C_PUCT = 1.0\n",
        "EVAL_CNT = 10"
      ],
      "metadata": {
        "id": "laPW7M5LyR7v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define Mcts class\n",
        "class Mcts:\n",
        "    def __init__(self, env, model, state, temperature):\n",
        "        self.env = copy.deepcopy(env)\n",
        "        self.model = model\n",
        "        self.state = state\n",
        "        self.temperature = temperature\n",
        "        self.legal_actions = self.env.check_legal_action(self.state)\n",
        "\n",
        "        # define Node class\n",
        "        class Node:\n",
        "            def __init__(self, mcts, state, p):\n",
        "                self.mcts = copy.deepcopy(mcts) # Mcts 객체 참조 저장\n",
        "                self.env = copy.deepcopy(self.mcts.env)\n",
        "\n",
        "                self.state = copy.deepcopy(state)\n",
        "                self.p = p # policy\n",
        "                self.n = 0 # count\n",
        "                self.w = 0 # cumulate value\n",
        "                self.child_nodes = None\n",
        "\n",
        "            def evaluate(self):\n",
        "                self.state[[0, 1]] = self.state[[1, 0]]\n",
        "                is_done, is_win = self.env.is_done(self.state)\n",
        "\n",
        "                # 게임 종료 시 승패에 따라 value 계산\n",
        "                if is_done:\n",
        "                    value = 1 if is_win else 0\n",
        "\n",
        "                    self.w += value\n",
        "                    self.n += 1\n",
        "                    return value\n",
        "\n",
        "                # child node가 없는 경우 => 확장\n",
        "                if not self.child_nodes:\n",
        "                    state = self.state\n",
        "                    state = torch.tensor(state, dtype = torch.float32)\n",
        "                    state = state.unsqueeze(0)\n",
        "                    # model을 통해 policy와 value 얻음\n",
        "                    policies, value = self.predict(state)\n",
        "\n",
        "                    self.w += value\n",
        "                    self.n += 1\n",
        "\n",
        "                    # expand child node\n",
        "                    self.child_nodes = []\n",
        "                    legal_actions = np.where(self.mcts.legal_actions != 0)\n",
        "\n",
        "                    for action, policy in zip(*legal_actions, *policies):\n",
        "                        self.env.present_state = copy.deepcopy(self.state)\n",
        "                        next_state, _, _, _ = self.env.step(action)\n",
        "                        self.child_nodes.append(Node(self.mcts, next_state, policy))\n",
        "\n",
        "                    return value\n",
        "\n",
        "                # end node가 아니고, child node가 있는 경우 => 전개\n",
        "                else:\n",
        "                    next_child_node = self.next_child_node()\n",
        "                    value = -next_child_node.evaluate()\n",
        "\n",
        "                    self.w += value\n",
        "                    self.n += 1\n",
        "                    return value\n",
        "\n",
        "\n",
        "            def next_child_node(self):\n",
        "                '''\n",
        "                PUCB에 따라 child node를 선택\n",
        "                '''\n",
        "                node_scores = list(map(lambda c: c.n, self.child_nodes))\n",
        "\n",
        "                scores = sum(node_scores)\n",
        "                # pucb 값에 따라 정렬한 child nodes list (마지막이 최댓값을 갖는 child node)\n",
        "                pucb_sorted = sorted(self.child_nodes, key = lambda c: (-c.w / c.n if c.n else 0.0) + C_PUCT * c.p * sqrt(scores) / (1 + c.n))\n",
        "\n",
        "                return pucb_sorted[-1]\n",
        "\n",
        "\n",
        "            def predict(self, state):\n",
        "                '''\n",
        "                model을 통해 policy와 value 계산\n",
        "                '''\n",
        "                x = state # 차원 맞추기\n",
        "                # x = x.unsqueeze(0) 아마도...\n",
        "                policies, value = self.mcts.model.forward(x)\n",
        "                policies = policies.detach().numpy()\n",
        "                value = value.detach().numpy()\n",
        "                policies = policies * self.mcts.legal_actions # legal action에 대한 policy\n",
        "                policies /= np.sum(policies) if np.sum(policies) else 1 # 합계 1의 확률분포로 변환\n",
        "\n",
        "                return policies, value\n",
        "\n",
        "        self.Node = Node # Node 객체 생성\n",
        "\n",
        "    ########################\n",
        "    # methods of Mcts\n",
        "    def get_policy(self, state):\n",
        "        '''\n",
        "        MCTS에 따라 policy 계산\n",
        "        '''\n",
        "        root_node = self.Node(self, state, 0) # Mcts 객체 self 전달\n",
        "\n",
        "        for i in range(EVAL_CNT):\n",
        "            root_node.evaluate()\n",
        "\n",
        "        scores = [c.n for c in root_node.child_nodes]\n",
        "\n",
        "        if self.temperature == 0: # 최대값인 경우에만 1로 지정\n",
        "            action = np.argmax(scores)\n",
        "            scores = np.zeros(len(scores))\n",
        "            scores[action] = 1\n",
        "\n",
        "        else: # 볼츠만 분포를 기반으로 분산 추가\n",
        "            scores = self.boltzman(scores, self.temperature)\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "    def boltzman(self, xs, temperature):\n",
        "        '''\n",
        "        볼츠만 분포\n",
        "        '''\n",
        "        xs = [x ** (1/temperature) for x in xs]\n",
        "        return [x/sum(xs) for x in xs]\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        '''\n",
        "        MCTS를 통해 얻은 policy에 따른 action 선택\n",
        "        '''\n",
        "        legal_actions = np.where(self.legal_actions != 0)[0]\n",
        "        policy = self.get_policy(state)\n",
        "        action = np.random.choice(legal_actions, p=policy)\n",
        "        return policy, action"
      ],
      "metadata": {
        "id": "kvxT4b6wzvE3"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04. Self-play"
      ],
      "metadata": {
        "id": "NNj6fQD3yTY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter\n",
        "SP_GAME_COUNT = 3  # 셀프 플레이를 수행할 게임 수(오리지널: 25,000)\n",
        "SP_TEMPERATURE = 1.0  # 볼츠만 분포의 온도 파라미터\n",
        "\n",
        "CONV_UNITS = 64\n",
        "\n",
        "state_size = (3,3)\n",
        "env = Environment(state_size)"
      ],
      "metadata": {
        "id": "AupHaIWJyWIn"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1번의 게임 play 함수\n",
        "def play_one_game(model):\n",
        "    env.reset()\n",
        "    done = False\n",
        "    history = []\n",
        "\n",
        "    while not done:\n",
        "        state = env.present_state.copy()\n",
        "        legal_actions = np.where(env.check_legal_action(env.present_state))\n",
        "        # state 순서: player = True 기준, (player_state, enemy_state) 고정\n",
        "        # env.render(state) # test를 위해\n",
        "        state[[0, 1]] = state[[1, 0]] if not env.player else state[[0, 1]]\n",
        "\n",
        "        mcts = Mcts(env, model, state, temperature = SP_TEMPERATURE)\n",
        "        scores, action = mcts.get_action(state)\n",
        "        _, reward, done, _ = env.step(action)\n",
        "\n",
        "        policies = [0.0]*env.num_actions\n",
        "\n",
        "        for action, policy in zip(*legal_actions, scores):\n",
        "            policies[action] = policy\n",
        "        # print(done)\n",
        "\n",
        "        history.append((state, policies))\n",
        "\n",
        "    history = [(x[0], x[1], reward) for x in history]\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "VsevBJtbz8cF"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self play 함수\n",
        "def self_play(model):\n",
        "    env = Environment(state_size)\n",
        "    data = []\n",
        "\n",
        "    for i in range(SP_GAME_COUNT):\n",
        "        history = play_one_game(model)\n",
        "        data.extend(history)\n",
        "        if i % 10 == 0:\n",
        "            print(f\"game {i+1}\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "mOkhZZkVz-Op"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "4d3-vZ7V3XmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = (3,3)\n",
        "env = Environment(state_size)\n",
        "model = Net(state_size, env.num_actions, CONV_UNITS)"
      ],
      "metadata": {
        "id": "olaam6Tb3Rdi"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = self_play(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHI_yHncuwkf",
        "outputId": "37bcef43-7955-4190-8a18-860c4667faae"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "game 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(hist))\n",
        "print(hist[-3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N07It6u1zPMr",
        "outputId": "99aac76a-586a-4ad0-f58f-77376e54ed0c"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "(array([[[-1.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.],\n",
            "        [ 0., -1., -1.]],\n",
            "\n",
            "       [[ 0.,  0., -1.],\n",
            "        [-1.,  0.,  0.],\n",
            "        [-1.,  0.,  0.]]]), [0.0, 0.2222222222222222, 0.0, 0.0, 0.3333333333333333, 0.4444444444444444, 0.0, 0.0, 0.0], 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05. Evaluate Network"
      ],
      "metadata": {
        "id": "HG-4blxzyi7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter\n",
        "NUM_GAME = 10\n",
        "TEMPERATURE = 1.0 # 볼츠만 분포\n",
        "\n",
        "file_name = \"model1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "v70qi70EyoNa"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 game play하는 함수\n",
        "def play_game(mcts_list):\n",
        "    env = Environment(state_size)\n",
        "\n",
        "    while not env.done:\n",
        "        state = env.present_state.copy()\n",
        "\n",
        "        next_player = mcts_list[0] if env.player else mcts_list[1]\n",
        "        _, action = next_player.get_action()\n",
        "        _, reward, _, _ = env.step(action)\n",
        "\n",
        "    point = 1 if reward==1 else 0.5 if reward==-1 else 0\n",
        "    return point # first player point"
      ],
      "metadata": {
        "id": "EQCfYZwP0Gno"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network 평가하는 함수\n",
        "def evaluate_network():\n",
        "    model_latest = Net(state_size, env.num_actions, CONV_UNITS).to(device)\n",
        "    model_best = Net(state_size, env.num_actions, CONV_UNITS).to(device)\n",
        "\n",
        "    with open(f'{file_name}_model_latest.pkl', 'rb') as f:\n",
        "        model_latest.load_state_dict(pickle.load(f))\n",
        "\n",
        "    with open(f'{file_name}_model_best.pkl') as f:\n",
        "        model_best.load_state_dict(pickle.load(f))\n",
        "\n",
        "    mcts_latest = Mcts(env, model_latest, env.present_state, TEMPERATURE)\n",
        "    mcts_best = Mcts(env, model_best, env.present_state, TEMPERATURE)\n",
        "\n",
        "    mcts_list = [mcts_latest, mcts_best]\n",
        "\n",
        "    # 대전\n",
        "    total_point = 0\n",
        "    for i in range(NUM_GAME):\n",
        "        # 선 플레이어를 교대하면서 대전\n",
        "        if i % 2 == 0: # first player: latest\n",
        "            point = play_game(mcts_list)\n",
        "\n",
        "        else: # first player: best\n",
        "            mcts_list[[0, 1]] = mcts_list[[1, 0]]\n",
        "            point = 1 - play_game(mcts_list) # latest의 point\n",
        "\n",
        "        total_point += point\n",
        "\n",
        "    average_point = total_point/NUM_GAME\n",
        "    print(f\"Average point: {average_point}\")\n",
        "\n",
        "    # best player 교체\n",
        "    if average_point > 0.5:\n",
        "        with open(f'{file_name}_model_best.pkl') as f:\n",
        "            pickle.dump(model_latest, f)\n",
        "\n",
        "        return True\n",
        "\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "eFXpJmmH0MDi"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. Train"
      ],
      "metadata": {
        "id": "R5btd4_PyWdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter\n",
        "TRAIN_NUM = 10\n",
        "TRAIN_EPOCHS = 100  # 학습 횟수\n",
        "BATCHSIZE = 64\n",
        "LEARN_MAX = 0.001\n",
        "\n",
        "SP_GAME_COUNT = 10\n",
        "\n",
        "file_name = \"model1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "state_size = (3,3)\n",
        "env = Environment(state_size)\n",
        "\n",
        "CONV_UNITS = 64\n",
        "model = Net(state_size, env.num_actions, CONV_UNITS).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_MAX, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "FUa0hBhIybSa"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function 정의\n",
        "def loss_function(pred_policy, pred_value, y):\n",
        "    mse = F.mse_loss(pred_policy, y[:, :-1])\n",
        "    cross_entrophy = -torch.mean(y[:, -2:-1] * torch.log(pred_value))\n",
        "    return mse + cross_entrophy"
      ],
      "metadata": {
        "id": "Fj3ze7JUyeO9"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset을 만드는 함수\n",
        "def make_dataset(history):\n",
        "    batch_size = min(BATCHSIZE, len(history))\n",
        "    mini_batch = random.sample(history, batch_size)\n",
        "    states, policies, results = zip(*mini_batch) # history에서 어떻게 생겼는지 봐야함\n",
        "    policies = np.array(policies)\n",
        "    results = np.array(results).reshape(-1, 1)\n",
        "    Y_array = np.concatenate([policies, results], axis=1)\n",
        "\n",
        "    X = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "    Y = torch.tensor(Y_array, dtype=torch.float32).to(device)\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "0mXR54iy0iTS"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network train하는 함수\n",
        "def train_network():\n",
        "    with open(f'{file_name}_history.pkl', 'rb') as f:\n",
        "        history = pickle.load(f)\n",
        "\n",
        "    for i in range(TRAIN_EPOCHS):\n",
        "        X, Y = make_dataset(history)\n",
        "        # 우선 그냥 현재의 model로 학습한다는 느낌...\n",
        "        pred_policy, pred_value = model.forward(X)\n",
        "        loss = loss_function(pred_policy, pred_value, Y)\n",
        "        # 역전파\n",
        "        optimizer.zero_grad()\n",
        "        loss.requires_grad_(True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 최근 모델 저장\n",
        "    with open(f'{file_name}_model_latest.pkl', 'wb') as f:\n",
        "        pickle.dump(model.state_dict(), f)\n",
        "\n",
        "    # lr,... epoch,... 조절..."
      ],
      "metadata": {
        "id": "20ObCEDG0lL8"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network train하는 함수\n",
        "def train_network(model, history):\n",
        "    for i in range(TRAIN_EPOCHS):\n",
        "        X, Y = make_dataset(history)\n",
        "        # 우선 그냥 현재의 model로 학습한다는 느낌...\n",
        "        pred_policy, pred_value = model.forward(X)\n",
        "        loss = loss_function(pred_policy, pred_value, Y)\n",
        "        # 역전파\n",
        "        optimizer.zero_grad()\n",
        "        loss.requires_grad_(True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # lr,... epoch,... 조절..."
      ],
      "metadata": {
        "id": "vxZ6-ysSO1qf"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = (3,3)\n",
        "env = Environment(state_size)\n",
        "\n",
        "CONV_UNITS = 64\n",
        "model = Net(state_size, env.num_actions, CONV_UNITS).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_MAX)\n",
        "\n",
        "# network train cycle\n",
        "for i in range(TRAIN_NUM):\n",
        "    history = self_play(model)\n",
        "    train_network(model, history)\n",
        "\n",
        "    env = Environment(state_size)\n",
        "    env.reset()\n",
        "\n",
        "    while not env.done:\n",
        "        if env.player:\n",
        "            state = env.present_state.copy()\n",
        "            state = torch.tensor(state, dtype = torch.float32)\n",
        "            state = state.unsqueeze(0)\n",
        "\n",
        "            policy, value = model.forward(state)\n",
        "            policy = policy.detach().numpy()\n",
        "            policy = policy * env.check_legal_action(env.present_state)\n",
        "            print(policy)\n",
        "            action = np.argmax(policy)\n",
        "        else:\n",
        "            legal_actions = np.where(env.check_legal_action(env.present_state)!=0)[0]\n",
        "            action = random.choice(legal_actions)\n",
        "\n",
        "        _, reward, done, is_win = env.step(action)\n",
        "\n",
        "        print(reward, done, is_win, env.player)\n",
        "        env.render(env.present_state)\n",
        "\n",
        "    # update_best_player = evaluate_network()\n",
        "\n",
        "    # if update_best_player:\n",
        "    #     evaluate_best_player()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0DOb3iuZ0qnk",
        "outputId": "ccb72b03-a9f4-4dde-ffc7-a59297ae2485"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "game 1\n",
            "[[0.09430505 0.08764277 0.09351463 0.09197062 0.12300365 0.14932774\n",
            "  0.09552693 0.12627751 0.13843104]]\n",
            "0 False False False\n",
            ". . . \n",
            ". . X \n",
            ". . .\n",
            "----------\n",
            "0 False False True\n",
            ". . . \n",
            ". O X \n",
            ". . .\n",
            "----------\n",
            "[[0.09430505 0.08764277 0.09351463 0.09197062 0.         0.\n",
            "  0.09552693 0.12627751 0.13843104]]\n",
            "0 False False False\n",
            ". . . \n",
            ". O X \n",
            ". . X\n",
            "----------\n",
            "0 False False True\n",
            ". . . \n",
            ". O X \n",
            "O . X\n",
            "----------\n",
            "[[0.09430505 0.08764277 0.09351463 0.09197062 0.         0.\n",
            "  0.         0.12627751 0.        ]]\n",
            "0 False False False\n",
            ". . . \n",
            ". O X \n",
            "O X X\n",
            "----------\n",
            "-1 True True True\n",
            ". . O \n",
            ". O X \n",
            "O X X\n",
            "----------\n",
            "game 1\n",
            "[[0.09577799 0.07429262 0.10749502 0.10580871 0.12213466 0.14040369\n",
            "  0.08962057 0.13660912 0.1278577 ]]\n",
            "0 False False False\n",
            ". . . \n",
            ". . X \n",
            ". . .\n",
            "----------\n",
            "0 False False True\n",
            ". . . \n",
            ". . X \n",
            ". . O\n",
            "----------\n",
            "[[0.09577799 0.07429262 0.10749502 0.10580871 0.12213466 0.\n",
            "  0.08962057 0.13660912 0.        ]]\n",
            "0 False False False\n",
            ". . . \n",
            ". . X \n",
            ". X O\n",
            "----------\n",
            "0 False False True\n",
            "O . . \n",
            ". . X \n",
            ". X O\n",
            "----------\n",
            "[[0.         0.07429262 0.10749502 0.10580871 0.12213466 0.\n",
            "  0.08962057 0.         0.        ]]\n",
            "0 False False False\n",
            "O . . \n",
            ". X X \n",
            ". X O\n",
            "----------\n",
            "0 False False True\n",
            "O . . \n",
            ". X X \n",
            "O X O\n",
            "----------\n",
            "[[0.         0.07429262 0.10749502 0.10580871 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "0 False False False\n",
            "O . X \n",
            ". X X \n",
            "O X O\n",
            "----------\n",
            "0 False False True\n",
            "O O X \n",
            ". X X \n",
            "O X O\n",
            "----------\n",
            "[[0.         0.         0.         0.10580871 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "1 True True False\n",
            "O O X \n",
            "X X X \n",
            "O X O\n",
            "----------\n",
            "game 1\n",
            "[[0.11706405 0.08788773 0.10303637 0.09938832 0.12682037 0.1297411\n",
            "  0.08995388 0.12032397 0.12578419]]\n",
            "0 False False False\n",
            ". . . \n",
            ". . X \n",
            ". . .\n",
            "----------\n",
            "0 False False True\n",
            ". . . \n",
            ". . X \n",
            ". . O\n",
            "----------\n",
            "[[0.11706405 0.08788773 0.10303637 0.09938832 0.12682037 0.\n",
            "  0.08995388 0.12032397 0.        ]]\n",
            "0 False False False\n",
            ". . . \n",
            ". X X \n",
            ". . O\n",
            "----------\n",
            "0 False False True\n",
            ". . . \n",
            "O X X \n",
            ". . O\n",
            "----------\n",
            "[[0.11706405 0.08788773 0.10303637 0.         0.         0.\n",
            "  0.08995388 0.12032397 0.        ]]\n",
            "0 False False False\n",
            ". . . \n",
            "O X X \n",
            ". X O\n",
            "----------\n",
            "0 False False True\n",
            ". . O \n",
            "O X X \n",
            ". X O\n",
            "----------\n",
            "[[0.11706405 0.08788773 0.         0.         0.         0.\n",
            "  0.08995388 0.         0.        ]]\n",
            "0 False False False\n",
            "X . O \n",
            "O X X \n",
            ". X O\n",
            "----------\n",
            "0 False False True\n",
            "X O O \n",
            "O X X \n",
            ". X O\n",
            "----------\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.08995388 0.         0.        ]]\n",
            "0 True False False\n",
            "X O O \n",
            "O X X \n",
            "X X O\n",
            "----------\n",
            "game 1\n",
            "[[0.11736854 0.09216832 0.09747909 0.10955323 0.13659483 0.11960658\n",
            "  0.09333056 0.10033119 0.13356765]]\n",
            "0 False False False\n",
            ". . . \n",
            ". X . \n",
            ". . .\n",
            "----------\n",
            "0 False False True\n",
            "O . . \n",
            ". X . \n",
            ". . .\n",
            "----------\n",
            "[[0.         0.09216832 0.09747909 0.10955323 0.         0.11960658\n",
            "  0.09333056 0.10033119 0.13356765]]\n",
            "0 False False False\n",
            "O . . \n",
            ". X . \n",
            ". . X\n",
            "----------\n",
            "0 False False True\n",
            "O . . \n",
            "O X . \n",
            ". . X\n",
            "----------\n",
            "[[0.         0.09216832 0.09747909 0.         0.         0.11960658\n",
            "  0.09333056 0.10033119 0.        ]]\n",
            "0 False False False\n",
            "O . . \n",
            "O X X \n",
            ". . X\n",
            "----------\n",
            "-1 True True True\n",
            "O . . \n",
            "O X X \n",
            "O . X\n",
            "----------\n",
            "game 1\n",
            "[[0.09919726 0.08827562 0.10661721 0.10660594 0.1316008  0.11412896\n",
            "  0.11829598 0.10067838 0.1345998 ]]\n",
            "0 False False False\n",
            ". . . \n",
            ". . . \n",
            ". . X\n",
            "----------\n",
            "0 False False True\n",
            ". . . \n",
            ". . . \n",
            ". O X\n",
            "----------\n",
            "[[0.09919726 0.08827562 0.10661721 0.10660594 0.1316008  0.11412896\n",
            "  0.11829598 0.         0.        ]]\n",
            "0 False False False\n",
            ". . . \n",
            ". X . \n",
            ". O X\n",
            "----------\n",
            "0 False False True\n",
            "O . . \n",
            ". X . \n",
            ". O X\n",
            "----------\n",
            "[[0.         0.08827562 0.10661721 0.10660594 0.         0.11412896\n",
            "  0.11829598 0.         0.        ]]\n",
            "0 False False False\n",
            "O . . \n",
            ". X . \n",
            "X O X\n",
            "----------\n",
            "0 False False True\n",
            "O O . \n",
            ". X . \n",
            "X O X\n",
            "----------\n",
            "[[0.         0.         0.10661721 0.10660594 0.         0.11412896\n",
            "  0.         0.         0.        ]]\n",
            "0 False False False\n",
            "O O . \n",
            ". X X \n",
            "X O X\n",
            "----------\n",
            "0 False False True\n",
            "O O . \n",
            "O X X \n",
            "X O X\n",
            "----------\n",
            "[[0.         0.         0.10661721 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "1 True True False\n",
            "O O X \n",
            "O X X \n",
            "X O X\n",
            "----------\n",
            "game 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-272-25696efa97c1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-271-f27784fc60a2>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(model, history)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment(state_size)\n",
        "env.reset()\n",
        "\n",
        "while not env.done:\n",
        "    if env.player:\n",
        "        state = env.present_state.copy()\n",
        "        state = torch.tensor(state, dtype = torch.float32)\n",
        "        state = state.unsqueeze(0)\n",
        "\n",
        "        policy, value = model.forward(state)\n",
        "        policy = policy.detach().numpy()\n",
        "        policy = policy * env.check_legal_action(env.present_state)\n",
        "        print(policy)\n",
        "        action = np.argmax(policy)\n",
        "    else:\n",
        "        legal_actions = np.where(env.check_legal_action(env.present_state)!=0)[0]\n",
        "        action = random.choice(legal_actions)\n",
        "\n",
        "    _, reward, done, is_win = env.step(action)\n",
        "\n",
        "    print(reward, done, is_win, env.player)\n",
        "    env.render(env.present_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mrQckKM-dHO",
        "outputId": "7e1f617b-a489-4cda-9313-bf6203e79b47"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "[[0.11440203 0.08727124 0.08985619 0.10936214 0.10744875 0.11670816\n",
            "  0.1109485  0.10561733 0.15838566]]\n",
            "0 False False False\n",
            ". . . \n",
            ". . . \n",
            ". . X\n",
            "----------\n",
            "0 False False True\n",
            ". . O \n",
            ". . . \n",
            ". . X\n",
            "----------\n",
            "[[[ 0.  0.  0.]\n",
            "  [ 0.  0.  0.]\n",
            "  [ 0.  0. -1.]]\n",
            "\n",
            " [[ 0.  0. -1.]\n",
            "  [ 0.  0.  0.]\n",
            "  [ 0.  0.  0.]]]\n",
            "[[0.11440203 0.08727124 0.         0.10936214 0.10744875 0.11670816\n",
            "  0.1109485  0.10561733 0.        ]]\n",
            "0 False False False\n",
            ". . O \n",
            ". . X \n",
            ". . X\n",
            "----------\n",
            "0 False False True\n",
            ". . O \n",
            "O . X \n",
            ". . X\n",
            "----------\n",
            "[[[ 0.  0.  0.]\n",
            "  [ 0.  0. -1.]\n",
            "  [ 0.  0. -1.]]\n",
            "\n",
            " [[ 0.  0. -1.]\n",
            "  [-1.  0.  0.]\n",
            "  [ 0.  0.  0.]]]\n",
            "[[0.11440203 0.08727124 0.         0.         0.10744875 0.\n",
            "  0.1109485  0.10561733 0.        ]]\n",
            "0 False False False\n",
            "X . O \n",
            "O . X \n",
            ". . X\n",
            "----------\n",
            "0 False False True\n",
            "X O O \n",
            "O . X \n",
            ". . X\n",
            "----------\n",
            "[[[-1.  0.  0.]\n",
            "  [ 0.  0. -1.]\n",
            "  [ 0.  0. -1.]]\n",
            "\n",
            " [[ 0. -1. -1.]\n",
            "  [-1.  0.  0.]\n",
            "  [ 0.  0.  0.]]]\n",
            "[[0.         0.         0.         0.         0.10744875 0.\n",
            "  0.1109485  0.10561733 0.        ]]\n",
            "0 False False False\n",
            "X O O \n",
            "O . X \n",
            "X . X\n",
            "----------\n",
            "0 False False True\n",
            "X O O \n",
            "O . X \n",
            "X O X\n",
            "----------\n",
            "[[[-1.  0.  0.]\n",
            "  [ 0.  0. -1.]\n",
            "  [-1.  0. -1.]]\n",
            "\n",
            " [[ 0. -1. -1.]\n",
            "  [-1.  0.  0.]\n",
            "  [ 0. -1.  0.]]]\n",
            "[[0.         0.         0.         0.         0.10744875 0.\n",
            "  0.         0.         0.        ]]\n",
            "1 True True False\n",
            "X O O \n",
            "O X X \n",
            "X O X\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9FYq3pszyfBW"
      }
    }
  ]
}